<!DOCTYPE html>
<html>
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Operating Systems &#8211; Tiger Shen</title>
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Lecture notes from Bradfield's operating systems class">
    <meta name="robots" content="all">
    <meta name="author" content="Tiger Shen">
    
    <meta name="keywords" content="">
    <link rel="canonical" href="http://tigerthinks.com/2017/03/26/bradfield-operating-systems/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Tiger Shen" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202011300937" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic&amp;subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300&amp;subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    
    <script type="text/javascript" async
        src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Operating Systems">
    <meta property="og:description" content="Tiger Shen">
    <meta property="og:url" content="http://tigerthinks.com/2017/03/26/bradfield-operating-systems/">
    <meta property="og:site_name" content="Tiger Shen">
    
    <meta property="og:image" content="http://tigerthinks.com/images/me.jpg">
    

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Operating Systems" />
    <meta name="twitter:description" content="Lecture notes from Bradfield's operating systems class" />
    <meta name="twitter:url" content="http://tigerthinks.com/2017/03/26/bradfield-operating-systems/" />
    
    <meta name="twitter:image" content="http://tigerthinks.com/images/me.jpeg" />
    

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="76x76" href="/favicon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">

    
</head>


<body>
  
  

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://tigerthinks.com/" class="site-title">Tiger Shen</a>
      <nav class="site-nav">
        <a href="/blog">
  Personal
</a>

<a href="/tech-blog">
  Tech
</a>

<a href="/anki">
  Anki
</a>

<a href="#">
  |
</a>

<a href="/books/top">
  Books (43)
</a>

<a href="/articles/top">
  Articles (119)
</a>

<a href="/other/top">
  Other (151)
</a>

      </nav>
      <div class="clearfix"></div>
      

      <span id="search-searchbar"></span>

    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        <div class="post-list" id="search-hits">
        </div>

        <div id="content">
          


<div class="post-header mb2">
  <h1>
    
    <div class="clearfix mxn2">
      <div class="col col-2 sm-col-12 px2">
        <img class="inline" src="/images//courses/bradfield.png" alt="Book Cover">

      </div>
      <div class="col col-10 sm-col-12 px2">
        Operating Systems
      </div>
    </div>
    
  </h1>
  <div class="tags">
  
    <a href="/course" class="badge badge-blue-grey-base">course</a>
  
    <a href="/technical" class="badge badge-deep-orange-base">technical</a>
  
    <a href="/cs" class="badge badge-deep-orange-100">cs</a>
  
    <a href="/bradfield" class="badge badge-amber-a200">bradfield</a>
  
</div>

  <span class="post-meta">By Bradfield | <a target="_blank" href="https://bradfieldcs.com/courses/operating-systems/">Course Page</a> | Taken 2017</span><br>
  
  <span class="post-meta small">
  
    73 minute read
  
  </span>
</div>

<article class="post-content">
  <b> <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20">  <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20">  <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20">  <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20"> </b>

  <h2 id="notes">Notes</h2>

<h3 id="1---introduction-to-operating-systems">1 - Introduction to Operating Systems</h3>

<h4 id="ostep-ch-2">OSTEP Ch 2</h4>

<ul>
  <li>Von Neumann: fetch/decode/execute hooray</li>
  <li>Operating system is responsible for making it easy to run programs,
allowing programs to share memory, enabling programs to interact
with devices, etc.</li>
  <li>Virtualization: OS takes physical resource and transforms into virtual
form that can be shared. OS is a VM</li>
  <li>Interface OS presents to users is in the form of system calls. System
calls make up the standard library of the OS</li>
  <li>OS is also a resource manager - manages CPU, memory, disk for all teh
running programs</li>
  <li>
    <p>Three major themes: virtualization, concurrency, persistence</p>
  </li>
  <li>Virtualizing the CPU: turning a single CPU into a seemingly infinite
number of CPUs from the program’s perspective</li>
  <li>Running multiple programs raises many hard problems. OS policies try
to resolve these problems consistently (e.g. two programs want to run
at same time, which gets precedence)</li>
  <li>
    <p>Memory is also virtualized! Each process has its own private virtual
address space, OS maps it onto physical memory</p>
  </li>
  <li>Concurrency is difficult</li>
  <li>Thread: function running within the same memory space as other
functions, with more than one of them active at a time</li>
  <li>
    <p>Problems b/c fetch decode execute are not atomic</p>
  </li>
  <li>Persistence :thinking_face:</li>
  <li>Memory is volatile, we need to be able to store stuff longer. Usually
in an SSD or hard drive</li>
  <li>
<em>file system</em> is the software in OS that manages the disk. Job is to
store files in reliable and efficient manner</li>
  <li>Writing file to disk is very ugly, OS abstracts it in the standard
library</li>
  <li>
    <p>Lots of ways to try and make it so we can recover from crash in middle
of write</p>
  </li>
  <li>Design goals time</li>
  <li>Abstractions pls. Pick the right abstractions to create an easy to use
system</li>
  <li>Performance is also important. OS overhead should be minimal</li>
  <li>
    <p>Protection between applications, and b/t OS and applications,
important to keep computer secure. Principle of <em>isolation</em> to make
sure that processes can’t interfere in bad ways</p>
  </li>
  <li>History</li>
  <li>Good ideas accumulate over time</li>
  <li>Started with a computer operator operating the computer</li>
  <li>Syscall vs procedure call: syscall transfers control to the OS while
raising the hardware privelege level. Syscall: trap -&gt; OS trap handler
-&gt; now in kernel mode -&gt; service request -&gt; return-from-trap -&gt; home</li>
</ul>

<h4 id="xv6-ch-0-1-3-section-on-syscalls">xv6 Ch 0-1, 3 section on syscalls</h4>

<ul>
  <li>xv6 is a trimmed down version of Unix that is good for learning</li>
  <li>
<em>kernel</em> defined here: special program that provides services to
running programs in the OS</li>
  <li>
    <p>Shell is in user space, which shows how powerful the syscall interface
is</p>
  </li>
  <li>Processes/memory</li>
  <li>Processes consist of user-space memory (instructions, data, stack) +
pre-process state private to the kernel</li>
  <li>To share registers, kernel can save state and restore when you want to
resume a program’s execution</li>
  <li>Each process has a pid</li>
  <li>Forking a process creates a child with same memory as the parent. It
returns the child pid in the parent, and 0 in the child</li>
  <li>Running down a bunch of different syscalls rapid fire;
open/read/write, exec, etc.
    <ul>
      <li>The main loop reads the input on the command line using getcmd. Then
it calls fork, which creates a copy of the shell process. The parent
shell calls wait, while the child process runs the command. For ex-
ample, if the user had typed ‘‘echo hello’’ at the prompt, runcmd would
have been called with ‘‘echo hello’’ as the argument. runcmd (8406) runs
the actual command. For ‘‘echo hello’’, it would call exec (8426). If
exec succeeds then the child will exe- cute instructions from echo
instead of runcmd. At some point echo will call exit, which will cause
the parent to return from wait in main</li>
    </ul>
  </li>
  <li>File descriptor: small int representing kernel-managed object a
process may read from or write to
    <ul>
      <li>Get one by opening a file, creating a pipe, duplicating existing
descriptor</li>
      <li>Abstracts diffs b/t pipes, files, devices; they all look like byte
streams!</li>
      <li>0 is stdin, 1 is stdout, and 2 is stderr</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">read(fd, buf, n)</code> reads up to n bytes from descriptor fd, copies them
into buf, returns number of bytes read. Write(fd, buf, n) is similar</li>
</ul>

<h4 id="lecture">Lecture</h4>

<ul>
  <li>Tradeoffs</li>
  <li>xv6: good for teaching, but slow and limited toolchain</li>
  <li>FreeBSD: comprehensive docs, dtrace is great, big tho</li>
  <li>
    <p>macOS: see your implementations and stuff</p>
  </li>
  <li>No nice abstractions like objects n stuff</li>
  <li>Instead, bit fields and such</li>
  <li>CPU reads/writes bytes from memory, moves them around</li>
  <li>Important jobs of OS: virtualization and memory protection</li>
  <li>Seg fault is because you’re trying to access part of the stack that
you haven’t asked for before (NOT because you’re trying to access
another program’s memory, since we’re virtualized)</li>
  <li>Container: just virtualizes the disk, you can read/write to new areas
and stuff</li>
  <li>
    <p>Virtual machine: present to the OS running on top of it what looks
like real hardware and it will run on top of that hardware</p>
  </li>
  <li>Unix</li>
  <li>Unix is command line operating system</li>
  <li>Difference between Unix/Linux/Posix</li>
  <li>Multics is an early OS from Bell, used for mainframe time sharing.
Multi process stuff</li>
  <li>
    <p>Unix was a side project from Thompson/Ritchie (1969). Everyone bought in
because C was so portable, could run easily on so many machines</p>
  </li>
  <li>v6 source got distributed a bunch, but AT&amp;T didn’t like it so they had
a lawsuit out so people stopped using “their” software</li>
  <li>Berkeley people took the source code and made BSD (Berkeley Software
DIstribution). AT&amp;T took offense. But BSD had already changed a bunch
of stuff around, so they just changed the remaining AT&amp;T code and
released a new version without any AT&amp;T code</li>
  <li>Stallman at MIT did the same thing and started the GNU project. Wrote
GNU in user land and a HURD microkernel in kernel land</li>
  <li>HURD sucked though (very slow), so Linus Torvalds took Stallman’s
userland programs (cd, ls, etc.) and wrote the Linux kernel for it</li>
  <li>NeXT took FreeBSD and renamed userland Darwin and renamed kernel Mach,
then Apple acquired them so that’s the basis for MacOS</li>
  <li>POSIX standardizes so you can reduce ifdefs in your C code
    <ul>
      <li>e.g. signature of a <code class="language-plaintext highlighter-rouge">read</code> call</li>
    </ul>
  </li>
</ul>

<h3 id="2---processes-and-the-process-api">2 - Processes and the Process API</h3>

<h4 id="ostep-ch-3-6">OSTEP Ch 3-6</h4>

<h5 id="ch-4-intro-to-processes">Ch 4: Intro to Processes</h5>

<ul>
  <li>Process is a fundamental abstraction. Definition? A running program</li>
  <li>OS creates illusion that there are many CPUs by <em>virtualizing</em> the
CPU: run one process, stop and run another, etc etc. This technique is
called <em>time sharing</em>
    <ul>
      <li>Time sharing: let resource be used for a little while by one entity,
then another, then another, it can be shared by many</li>
      <li>Corollary is space-sharing, e.g. memory</li>
    </ul>
  </li>
  <li>Time sharing is a <em>mechanism</em>; mechanisms are low-level methods or
protocols that implement a piece of functionality</li>
  <li>
<em>Policies</em> live on top of mechanisms. Policies are software for making
decisions in the OS, e.g. scheduling</li>
  <li>
    <p>Mechanism is <em>how</em> you accomplishes something (<em>how</em> do you context
switch?), policy decides <em>which</em> is appropriate (which process should
be executed right now)</p>
  </li>
  <li>
<em>Process</em>: OS abstraction for running program</li>
  <li>
    <p>What’s in a process? Its machine state. Includes memory (its address
space), PC, registers, I/O information, etc.</p>
  </li>
  <li>The Process API</li>
  <li>
<em>create</em>: create a process</li>
  <li>
<em>destroy</em>: kill a process</li>
  <li>
<em>wait</em>: wait for process to stop</li>
  <li>
<em>miscellaneous control</em>: suspend and resume, etc.</li>
  <li>
    <p><em>status</em>: how u doin rn</p>
  </li>
  <li>Creation</li>
  <li>First, load program’s code and static data into memory in the
process’s address space. Usually program is on disk in executable
format
    <ul>
      <li>Generally <em>lazy</em> (only load what you need)</li>
      <li>Uses mechanisms of paging and swapping</li>
    </ul>
  </li>
  <li>Then, allocate memory for the stack and heap</li>
  <li>Other misc. initialization, e.g.  set up 0, 1, 2 file descriptors</li>
  <li>
    <p>Now you can go into the main() routine of the program dn begin
execution</p>
  </li>
  <li>What states can a process be in?</li>
  <li>
<em>Running</em>: it’s running on the processor</li>
  <li>
<em>Ready</em>: it’s ready to be run, but scheduler holding for some reason</li>
  <li>
<em>Blocked</em>: waiting on something else to happen first. Commonly,
waiting for i/o request to disk</li>
  <li>State transitions: from running to blocked is initiate I/O, blocked to
ready is done I/O, between running and ready is schedule/deschedule</li>
  <li>When process is blocked, CPU can run another processo</li>
  <li>
<em>process list</em>: all ready processes, and some metadata on what’s
currently running. Also need to know what’s blocked</li>
  <li>Each process list entry has the register context (what’s in the
registers of the stopped process) for context switching</li>
</ul>

<h5 id="ch-5-process-api">Ch 5: Process API</h5>

<ul>
  <li>
    <p>fork, exec, wait are Unix syscalls for process creation</p>
  </li>
  <li>Fork is really strange at first glance</li>
  <li>Need to do conditional logic on the return value to determine if in
child or parent</li>
  <li>Returns child pid to parent, returns 0 to child, returns -1 if fork
failed</li>
  <li>PID in Unix is the process identifier, used to name/identify the
process</li>
  <li>
    <p>Ordering is non-deterministic of whether child or parent goes first.
Need to call wait() from parent to wait for child to finish, in order
to make fork execution deterministic</p>
  </li>
  <li>exec() calls a different program (whereas fork invokes same program)</li>
  <li>Given the name of an executable, it loads code from that executable,
overwrites its current code/data segments with it, re-initializes heap
and stack, and then run that program with the passed-in args as the ARGV
of that process</li>
  <li>Does <em>not</em> create a new process; instead, transforms current program
into different program</li>
  <li>
    <p>Lampson’s Law: Get it right. Not pretty, simple, abstract, w/e. Right.</p>
  </li>
  <li>Using fork and exec to build a shell: important because shell can run
code after fork() but before exec()</li>
  <li>Redirecting output:</li>
  <li>When creating child, before calling exec(), close STDOUT, open
newfile.txt, and then anything from the program’s STDOUT will be
redirected to file. Wow. Literally close(STDOUT_FILENO) and
open(“./p4_output”)</li>
  <li>Pipes similar to redirect: one process’ output is redirected to an
in-kernel pipe, another process’ input is connected to same pipe</li>
  <li>
    <p>Lots more to learn, this is just scratching the surface</p>
  </li>
  <li>kill() kills process</li>
  <li>top/ps for analyzing current processes</li>
</ul>

<h5 id="ch-6-mechanism-limited-direct-execution">Ch 6: Mechanism: Limited Direct Execution</h5>

<ul>
  <li>
    <p>Some challenges in time sharing. Performance? Control? Security?</p>
  </li>
  <li><em>Limited Direct Execution</em></li>
  <li>Direct execution: just run program on CPU</li>
  <li>Problem with direct execution? <em>Security</em> (need process to be able to
do kernel level stuff without kernel level priveleges) and <em>context
switching</em> (how do?)</li>
  <li>Limited: put clamps on process permissions</li>
  <li>
<em>trap</em> instruction is what C uses to turn stuff like open() into a
syscall</li>
  <li>Introduce permissions via user mode (restricted) and kernel mode (what
the OS is in)</li>
  <li>Syscalls are interface between user mode and kernel mode</li>
  <li>Kernel code locations and jumps and stuff are configured when the CPU
boots up</li>
  <li>trap will jump into the kernel code and set permissions to kernel mode</li>
  <li>When kernel code done executing, OS calls return-from-trap, which
returns and sets privilege mode back to user</li>
  <li>Process state (PC, flags, registers) saved onto the kernel stack when
kernel code is called</li>
  <li>
    <p>How does kernel know which code is kernel code? Set up a <em>trap table</em>
at boot time, telling CPU what code to run when signals get trapped</p>
  </li>
  <li>How does the OS regain control of the CPU when a process is running,
so that it can switch between processes?</li>
  <li>Cooperative: wait for a syscall from program to take back control.
Doesn’t really work if process doesn’t cooperate. Infinite loop =
restart, oops</li>
  <li>Non-cooperative: <em>timer interrupt</em>. Timer device interrupts every X
milliseconds, and OS interrupt handler runs to see if any changes are
needed. OS tells hardware what to run during timer interrupt at boot
time. OS also must start the timer</li>
  <li>Hardware’s job to save system state before timer interrupt so it can
return from trap</li>
  <li>Context switching is basically switching the registers to those of
another process (stored in process list) and the kernel stack of the
other process</li>
  <li>Maybe come back and look at this later?</li>
  <li>
    <p>How long do context switches take? Sub-microsecond on 2-3 GHz
processors, yay</p>
  </li>
  <li>
    <p>Concurrency? What if you’re in the timer interrupt and another one
happens? Maybe disable interrupts while handling one? Put locks on
internal data structures?</p>
  </li>
  <li>Cool, so limited direct execution!</li>
  <li>Analogous to baby-proofing a room; the OS baby-proofs the CPU :)</li>
</ul>

<h4 id="xv6-ch-5">xv6 Ch 5</h4>

<ul>
  <li>Skim now (before lecture), come back and read later</li>
</ul>

<h4 id="lecture-1">Lecture</h4>

<ul>
  <li>All processes in process table also store ppid (parent process id).
First process started by OS is pid 0, which initially forks as process
1</li>
  <li>Fork shares memory and stuff and lots of stuff in the child</li>
  <li>
    <p>Daemonize: don’t kill the child when the parent dies. Assign the child
to the original process before killing the parent so that it doesn’t
kill itself</p>
  </li>
  <li>
    <p>Data Oriented Programming is how all OS programming works. Need to
increase throughput using data and by remembering how hardware
actually works</p>
  </li>
  <li>dtrace time</li>
  <li>Bryan Cantrill was not pleased with the state of debuggers - either
print debugging or step through debugging, but both of them have their
own costs</li>
</ul>

<h3 id="3---scheduling-strategies">3 - Scheduling Strategies</h3>

<h4 id="ostep-ch-7---12">OSTEP Ch 7 - 12</h4>

<h5 id="ch-7-scheduling-introduction">Ch 7: Scheduling: Introduction</h5>

<ul>
  <li>Context switching a mechanism, scheduling a policy</li>
  <li>
    <p>Q: How do we develop a framework for thinking about scheduling? What
are assumptions/metrics/approaches?</p>
  </li>
  <li>
<em>workload</em>: all the processes (jobs) running in system. Assumptions:
    <ul>
      <li>Each job runs same amt. of time</li>
      <li>All jobs arrive same time</li>
      <li>Each job runs to completion once started</li>
      <li>All jobs only use CPU</li>
      <li>Run-time of each job is known</li>
    </ul>
  </li>
  <li>
    <p>Unrealistic!</p>
  </li>
  <li>
<em>Scheduling metric</em>: how to measure stuff. Use <em>turnaround time</em>: time
@ which job completes - time @ which job arrived in system
    <ul>
      <li>This is completeness metric; can also measure fairness</li>
    </ul>
  </li>
  <li>FIFO, or First Come First Served. Like queue</li>
  <li>Simple, not great when you have long-running jobs. Increases average
turnaround by a lot. This is the <em>convoy effect</em>: # of short
jobs get queued behind heavy, long job</li>
  <li>
    <p>Relax assumption that ea. job runs to completion once started</p>
  </li>
  <li>SJF: Shortest Job First. Take shortest one. Used IRL, like express
lines at grocery markets. Solves convoy effect</li>
  <li>Schedulers are <em>preemptive</em> today: will stop in middle of running
process in order to run another. Opposite is only re-scheduling when
current process is done</li>
  <li>
    <p>K, now what if jobs arrive at different times? Can still get convoy
effect if short comes after long</p>
  </li>
  <li>Then use STCF: Shortest Time-to-Completion First. During timer
interrupt, see which one has longest time left</li>
  <li>
    <p>Shit, what about interactivity? Need <em>response</em> time: T(first run of
program) - T(arrival of program)</p>
  </li>
  <li>Round Robin runs job for <em>time slice</em> then switches to next job in
queue. Keep jobs in priority queue. Need to choose a good time slice:
too high means higher response time, too low means context switching
cost will dominate. Want it long enough to amortize switching cost</li>
  <li>
<em>Fair</em>: evenly divides CPU among active processes on small time scale</li>
  <li>Round Robin = low response, high turnaround. SJF = high response,
low turnaround</li>
  <li>
    <p>What about I/O?</p>
  </li>
  <li>During I/O process is blocked waiting for I/O completion. Should
schedule another job</li>
  <li>After I/O is done, process moved from block to ready by interrupt</li>
  <li>Can treat each separated-by-I/O portion as a sub-job that gets
re-enqueued, and process seperately. You can get overlap this way.
Nice</li>
  <li>Okay, what if you don’t know how long job will take? Shit</li>
</ul>

<h5 id="ch-8-multi-level-feedback-queue">Ch 8: Multi-Level Feedback Queue</h5>

<ul>
  <li>Rule 1: if Priority(A) &gt; Priority(B), A runs and B doesn’t</li>
  <li>Rule 2: If Priority(A) = Priority(B), A &amp; B run in RR</li>
  <li>Rule 3: When job enters system, it gets highest priority</li>
  <li>Rule 4a: If job uses entire time slice, its priority is reduced</li>
  <li>Rule 4b: If job gives up CPU before time slice is up (waiting, I/O),</li>
  <li>Rule 4 (combined): Once a job uses up its time allotment at a given
level (regardless of # of times it has given up CPU), its priority is
reduced</li>
  <li>
    <p>Rule 5: After some time period, move all jobs in system to topmost
queue (Boost)</p>
  </li>
  <li>Multi-level Feedback Queue approach (MLFQ). Won Corbato a Turing award
in 60s</li>
  <li>Problem: optimize turnaround time while minimizing response time</li>
  <li>Scheduler needs to learn through context over time; won’t have perfect
knowledge of job length beforehand.</li>
  <li>
    <p>Learn-from-history approach good when jobs are chunked up and
predictable. Break it down</p>
  </li>
  <li>MLFQ has different <em>queues</em> with different <em>priority levels</em>. Ready
job on one queue at a time</li>
  <li>Set priority using observed behavior heuristics :thinking_face:</li>
  <li>If process waits for keyboard input, priority should be high, want to
prioritize response time</li>
  <li>
    <p>Approximate SJF, since you <em>assume</em> job is short-running until you
have reason to believe it’s long running. Okay</p>
  </li>
  <li>Problem: program behavior can change over time. Processes can
get <em>starved</em>, RIP</li>
  <li>
    <p>Rule 5! Takes out starvation, and addresses changed behavior</p>
  </li>
  <li>Problem: people can game scheduler and take up 99% of CPU time by
never using a whole time slice</li>
  <li>
    <p>Rule 4 combined. Different CPU time <em>accounting</em>. Prevents process
from sticking in top level queue</p>
  </li>
  <li>Bleh, PITA to tune this stuff. How many queues? What’s the time slice?</li>
  <li>Generally high priority queues get low time slice b/c interactive ok</li>
  <li>
    <p>Solaris uses a table of these values that can be alered. FreeBSD uses
algorithms to calculate current job priority based on how much CPU
it’s used</p>
  </li>
  <li>
    <p>Avoid magic numbers/constants</p>
  </li>
  <li>Can also give scheduler advice about how long you’re gonna take and
stuff.</li>
  <li>MLFQ observes behavior patterns of jobs and adjusts accordingly,
since it doesn’t have previous knowledge. This good</li>
</ul>

<h5 id="ch-9-proportional-share">Ch 9: Proportional Share</h5>

<ul>
  <li>Instead of optimizing turnaround/response, try and get all processes
to a certain % of CPOU time</li>
  <li>
<em>Tickets</em> represent the share of a resource a process should receive.
I.e. A has 75 tix B has 25 tix, A should get 75% of the time</li>
  <li>Random approach. Why is random good?
    <ul>
      <li>Avoids strange corner cases</li>
      <li>Lightweight</li>
      <li>Fast</li>
    </ul>
  </li>
  <li>Ticket mechanisms</li>
  <li>
<em>Currency</em>: give each user/process a set of global tickets which
they divide up. These are transferred to global tickets at scheduling
time to run lottery on who goes next</li>
  <li>
<em>Transfer</em>: process can temporarily hand off tickets to another
process, e.g. client passing tix to server to get work done faster</li>
  <li>
    <p><em>Inflation</em>: process that is trusted by others can give itself more
tickets to take priority</p>
  </li>
  <li>Implementation v simple: random number generator, process table, track
total number of tickets. Nice</li>
  <li>
    <p><em>Unfairness</em> (how long after first process ends does second process
end) is negligible over time</p>
  </li>
  <li>
    <p>Optimal ticket assignment is a tough cookie</p>
  </li>
  <li>Stride scheduling is alternate to lottery: each job has a pass value
(counter of how many times it’s run IG), and a stride value (total tix
/ your tix). Increment pass value by stride every time program runs,
run the lowest pass value every time. Ends up being proportional</li>
  <li>Needs to maintain global state though, not the play</li>
  <li>Deterministic</li>
  <li>MLFQ more common</li>
  <li>Use case: virtualizingaaa, where you know you want Windows VM to have
25% of CPU cycles and Linux to have 75%</li>
</ul>

<h5 id="ch-10-multiprocessor-scheduling">Ch 10: Multiprocessor Scheduling</h5>

<ul>
  <li>Skim now, come back later</li>
  <li>Scheduling is hard af</li>
  <li>Kay</li>
</ul>

<h4 id="lecture-2">Lecture</h4>

<ul>
  <li>
    <p>“Not Responding” in activity monitor = d e a d, taking all the cycles
and not responding to events</p>
  </li>
  <li>dtrace</li>
  <li>syscall:freebsd:execve:entry</li>
  <li>provider<img class="emoji" title=":package:" alt=":package:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e6.png" height="20" width="20">function_name:name</li>
  <li>
    <p>There are <code class="language-plaintext highlighter-rouge">sched</code> probes but I couldn’t quite get them to work</p>
  </li>
  <li>The process abstraction is quite powerful, scheduling is difficult</li>
</ul>

<h3 id="4---virtualizing-memory">4 - Virtualizing Memory</h3>

<h4 id="ostep-ch-13-address-spaces">OSTEP Ch 13: Address Spaces</h4>

<ul>
  <li>Every user address is virtual. Why? <em>Ease of use</em>. Each process gets
an <em>address space</em> to put its stuff in. Also isolation and protection
:)</li>
  <li>At first, no memory virtualization</li>
  <li>Then <em>multiprogramming</em>; CPU virtualization allowed concurrent
processes. How to share memory between processes as well? Allows stuff
like interactivity :thinking_face:</li>
  <li>Abstraction is <em>address space</em>: running program’s view of memory in
system
    <ul>
      <li>Contains text (code), data, heap, etc.</li>
    </ul>
  </li>
  <li>Problem this solves: multiple running processes need private address
spaces on top of single physical memory</li>
  <li>What are our <em>goals</em>?
    <ul>
      <li>
<em>Transparency</em>: running program shouldn’t realize anything’s going
on</li>
      <li>
<em>Efficiency</em>: as little performance impact as possible. Need
hardware support</li>
      <li>
<em>Protection</em>: processes should be protected from each other</li>
    </ul>
  </li>
  <li>
    <p>This is <em>virtualizing memory</em>: program thinks it’s at an address and
has a lot of accessible memory, but in reality it’s sharing</p>
  </li>
  <li>
    <p>Tip: isolation: one process can fail without affecting another.
Isolation of processes is a better dynamic</p>
  </li>
  <li>Again, <em>mechanism</em>s are low-level protocols or method for implementing
stuff (“how”), <em>policies</em> decide “which” decision to make</li>
</ul>

<h4 id="ostep-ch-14-interlude-memory-api">OSTEP Ch 14: Interlude: Memory API</h4>

<ul>
  <li>Hope some of these concepts are from CompArch</li>
  <li>Yeah I know most of this</li>
  <li>Stack variables are inside scopes. Stack == automatic memory</li>
  <li>
    <p><em>Heap</em> allocations are handled by you, the programmer. <em>malloc</em></p>
  </li>
  <li>
    <p>Don’t need to require #include <stdlib.h> but it allows compiler to
check if you're calling it correctly</stdlib.h></p>
  </li>
  <li>Best practice to cast result of malloc to be concrete about what
you’re doing: (double *) malloc(sizeof(double))</li>
  <li>
    <p>free() reverses malloc, cool</p>
  </li>
  <li>Ooh common errors</li>
  <li>Forget to allocate
    <ul>
      <li>SEGFAULT: tried to access memory you have not allocated yet</li>
    </ul>
  </li>
  <li>Not allocating enough. You’ll overwrite some poor other variable</li>
  <li>Make sure you fill in a value at the pointer you’ve malloc’d</li>
  <li>Forgetting to free. Oops, memory leak</li>
  <li>Free memory before you’re done: results in a dangling pointer, which
can now point to something you have no control over</li>
  <li>
    <p>Double free (freeing something already freed) can wrreck you too</p>
  </li>
  <li>Leaking memory in short-lived processes is “okay” because OS will
reclaim memory after process dies anyways</li>
  <li>
<code class="language-plaintext highlighter-rouge">brk</code> (set the location of the end of the heap) and <code class="language-plaintext highlighter-rouge">sbrk</code> (increment
heap pointer) are what malloc and free use under the hood. They are
syscalls</li>
  <li>Cool, there goes malloc and free</li>
</ul>

<h4 id="ostep-ch-15-mechanism-address-translation">OSTEP Ch 15: Mechanism: Address Translation</h4>

<ul>
  <li>Mechanism for CPU virtualization was <em>Limited Direction Execution</em>
</li>
  <li>Crux: How to Efficiently and Flexibly Virtualize Memory
    <ul>
      <li>Flexible: don’t restrict how programs use their own virtual memory
space</li>
    </ul>
  </li>
  <li>
    <p>Tip: <em>interposition</em>: transparently make a change behind the scenes
to make life easier for your client. Address translation is an example</p>
  </li>
  <li>Alright, let’s roll. Variables (start by assuming they’re static):
    <ul>
      <li>Is virtual memory a contiguous block in physical memory?</li>
      <li>Is virtual memory smaller than physical memory?</li>
      <li>Is each address space the same size?</li>
    </ul>
  </li>
  <li>Assume all these are static</li>
  <li>Virtual memory layout: code/text at the top, then heap directly below.
Stack grows up from bottom up to a certain size</li>
  <li>How to translate address 15kb in program to its place in hardware?</li>
  <li>
    <p>Aside: Software-based static relocation (loader rewrites executable to
have physical addresses before it runs) is worse; doesn’t have any
protection, processes can generate bad addresses and overwrite other
processes’ stuff</p>
  </li>
  <li>Dynamically (hardware-based)
    <ul>
      <li>
<em>base and bounds</em> (dynamic relocation): one CPU register has the
<em>base</em> of the physical memory location of the virtual memory of the
currently-running process, and another has the <em>bounds</em>. Thus,
physical address = virtal address + base</li>
      <li>Dynamic since it happens at runtime</li>
      <li>Bounds register makes sure addresses are within confines of address
space</li>
      <li>Having these values in registers is a big boost to performance</li>
      <li>MMU (memory management unit) is the part of the CPU that contains
this circuitry</li>
    </ul>
  </li>
  <li>Aside: <em>Free list</em> is the data structure the OS has to keep track of
which parts of physical memory have not yet been allocated</li>
  <li>Hardware support
    <ul>
      <li>Must support kernel (privileged) mode and user mode.</li>
      <li>Need the base and bounds registers on the MMU</li>
      <li>Ability to translate virtual addresses to physical ones</li>
      <li>Provide special instructions to modify base/bounds (only allowed in
kernel mode)</li>
      <li>Raise exceptions when program tries to access memory illegally, and
jump to the kernel exception handler</li>
    </ul>
  </li>
  <li>OS requirements
    <ul>
      <li>Memory management: allocate memory for new processes, reclaim memory
from terminated processes. Uses free list</li>
      <li>Base/bounds management: set base/bounds appropriately on context
switch. Keep these values in the process table
        <ul>
          <li>When process is stopped, can move its memory. Just copy the
address space and then update the base value in the process table</li>
        </ul>
      </li>
      <li>Exception handling: provide code to run when CPU raises exception.
Stuff like reclaiming its memory and cleaning up associated data
structures
        <ul>
          <li>The address of the exception handler is configured when trap table
is initialized at boot time</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>This concept <em>builds upon</em> limited direct execution, just save extra
stuff in the process context and add a bit more hardware</li>
  <li>Base and bounds approach
    <ul>
      <li>Efficient: done in hardware</li>
      <li>Protection: no process can access outside memory</li>
      <li>Wastes space that is not used by process. <em>Internal fragmentation</em>:
space inside an allocated unit is not used at all. Solve with
<em>segmentation</em>
</li>
    </ul>
  </li>
</ul>

<h4 id="ostep-ch-16-segmentation">OSTEP Ch 16: Segmentation</h4>

<ul>
  <li>Crux: How do you support a large address space with lots of space b/t
stack and heap?</li>
  <li>Separate base and bounds pair per logical segment of address space
(code, stack, heap)
    <ul>
      <li>Accomodate sparse address spaces</li>
    </ul>
  </li>
  <li>
    <p>Segmentation fault is when you access segmented machine at illegal
address. RIP</p>
  </li>
  <li>How to know which segment an address belongs two?</li>
  <li>Can reserve first two bits of address as segment ID, then rest as
offset. e.g. 00 = code, 01 = stack</li>
  <li>Can look at how address was generated (if from stack pointer, it’s
in stack, if from PC, it’s in code)</li>
  <li>Need to track whether address space/segment grows positively or
negatively (stack grows negatively). Just use a bit for this</li>
  <li>Add in protection bits to enable sharing. Read-write, read-execute,
read-only, etc. Thus, diff programs can look at same information,
think it’s their own, but in reality it’s shared
    <ul>
      <li>Add support to hardware for checking permission bits as well</li>
    </ul>
  </li>
  <li>Okay, now with segments, how do you allocate memory? Throw out
assumption that each segment is the same size, because they’re
obviously not</li>
  <li>Don’t want small holes of unusable free space (<em>external
fragmentation</em>)</li>
  <li>Compacting physical memory by keeping on rearranging segments is an
option, but bad because costly</li>
  <li>
    <p>Rather, use a free-list management algorithm that tries to keep large
pieces of memory available for allocation. Lots of options; no “best”
way to do this</p>
  </li>
  <li>Segmentation is good!</li>
  <li>Support sparse address spaces</li>
  <li>Performant: still in hardware</li>
  <li>Allows code sharing</li>
  <li>Still not flexible enough to support fully-generalized address space.
Large but sparsely-used heap in a logical segment leads to
fragmentation because entire heap still needs to be in memory</li>
  <li>Kay</li>
</ul>

<h4 id="ostep-ch-17-free-space-management">OSTEP Ch 17: Free-Space Management</h4>

<ul>
  <li>Alrighty. Hard to manage with variable-sized blocks</li>
  <li>How do you manage free space? What strategies and tradeoffs are there?</li>
  <li>Assumptions
    <ul>
      <li>malloc/free-like interface for users to manage memory</li>
      <li>free list data structure to keep track of free space
        <ul>
          <li>Literally a linked list of the contiguous blocks of free memory</li>
        </ul>
      </li>
      <li>Once memory is givent o client, it can’t be relocated to another
spot in memory</li>
      <li>No brk or sbrk - assume each region is a single fixed size
throughout its life</li>
    </ul>
  </li>
  <li>Mechanisms</li>
  <li>Splitting and coalescing: When you get a request for less memory than
one of your free chunks, split it up, return first part to the
requester, second part replaces previous chunk in free list
    <ul>
      <li>
<em>Coalesce</em>: when memory is freed and is right next to another free
chunk, merge then together</li>
    </ul>
  </li>
  <li>Allocator stores a header before the pointer it allocates that
identifies it so free(void *) can just take a pointer, doesn’t need to
know its length
    <ul>
      <li>Header has <em>size</em> and <em>magic</em> number to provide integrity checking</li>
      <li>Must search for space of size header + N bytes when allocating</li>
    </ul>
  </li>
  <li>Where do you keep free list in memory?</li>
  <li>Allocate a heap with mmap, set up your list, whenever you add stuff
you just move along your list
    <ul>
      <li>head points to the first free node. This location moves as you
allocate memory, <em>before</em> freeing it. This is contiguous</li>
      <li>When memory freed, library figures out size of free region, adds it
back to the free list by inserting at the head or something</li>
    </ul>
  </li>
  <li>How does OS grow the heap?</li>
  <li>Sometimes fail</li>
  <li>Sometimes find free physical pages, map them into address space of
requesting process, return value of end of new heap. Tight</li>
  <li>Skim some basic strategies for allocating memory</li>
  <li>All these strategies trade off cost of scaling (searching list) for
more complex in memory data structures and stuff. Simplicity for
performance</li>
</ul>

<h3 id="5---paging">5 - Paging</h3>

<h4 id="ostep-ch-18-intro-to-paging">OSTEP Ch 18: Intro to Paging</h4>

<ul>
  <li>If you use different sized chunks of memory you get fragmenting. So
why not use fixed-sized pieces? These are <em>pages</em>.</li>
  <li>View physical memory as array of page frames, each of which can
contain a single virtual-memory page</li>
  <li>So, how to virtualize memory with pages to avoid fragmentation? With
minimal time/space overhead</li>
  <li>Paging provides flexibility in the virtual memory abstraction :)</li>
  <li>
    <p>Also simplicity - OS just tracks free list of pages, not a bunch of
different types of free space</p>
  </li>
  <li>Use <em>page table</em> to track where each virtual page of address space is
placed in physical memory. This is per-process. Role of page table is
to store address translations for each page</li>
  <li>How to translate?</li>
  <li>First, get a virtual page number and an offset in the page. Can just
use separate bits in the virtual address to indicate these</li>
  <li>
    <p>Use the page table to translate the virtual page number into the
physical page number (PFN), and use that same offset, you can now go
fetch the data from physical memory</p>
  </li>
  <li>Where do you put page tables? They can get pretty darn big to do all
the necessary address translations</li>
  <li>Gotta put em in memory</li>
  <li>Linear page table: use virtual page number to index into array, looks
up the page-table entry (PTE) at that index, which should point to the
desired physical frame number. Ezpz</li>
  <li>Each page table entry can have some bits
    <ul>
      <li>Valid bit: is your transition valid? E.g. have you asked for this
heap</li>
      <li>Protection bits: permissions on read/write</li>
      <li>Present bit: is data on memory or on disk (swapped)</li>
      <li>Reference bit: has this page been accessed? Used for swapping and
such</li>
    </ul>
  </li>
  <li>This too slow and takes up too much space! Some pseudo-code here for
virtual memory paging</li>
  <li>Lots of extraneous memory accesses and such</li>
  <li>Okay, don’t fully understand the diagram</li>
</ul>

<h4 id="ostep-ch-19-paging-faster-translations-tlbs">OSTEP Ch 19: Paging: Faster Translations (TLBs)</h4>

<ul>
  <li>How do you speed up address translation and have it take up less memory?</li>
  <li>Add a <em>translation-lookaside buffer (TLB)</em> as part of the chip’s MMU,
which is a cache of virtual-to-physical address translations. Could
call it an address-translation cache</li>
  <li>Hardware first checks TLB before doing any computation to translate
addresses</li>
  <li>If it gets a hit, you can do the same checks on permissions and
validity and whatnot</li>
  <li>If no hit, then proceed as usual and also update the TLB</li>
  <li>Try to get as many hits as possible!</li>
  <li>e.g. sequential array accesses on the same page as memory are good.
This is spacial locality</li>
  <li>Hit rate: % of accesses that are TLB hits</li>
  <li>
    <p>If you re-reference the same stuff closely in time to each other, you
get temporal locality</p>
  </li>
  <li>Caching is a great performance technique in general :)</li>
  <li>At first hardware managed TLBs; had to know where on disk the page
tables were, and their format, to be able to do this. x86 did this
originally :)</li>
  <li>Nowadays we have software-managed TLBs. Hardware jumps to specific
spot in kernel code when TLB miss happens</li>
  <li>How to avoid infinite TLB? Have some permanently-valid translations in
the TLB (<em>wired</em>)</li>
  <li>
    <p>Software-managed TLBs are more flexible (any data type can be used)
and simple (hardware has less to do)</p>
  </li>
  <li>TLB contents are <em>fully associative</em>: any given translation can be
anywhere in the TLB, and hardware searches entire thing at any given
time</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Each entry: VPN</td>
          <td>PFN</td>
          <td>other bits</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>Other bits can be stuff like <em>valid bit</em> (contrast with page table
valid bit; page table one indicates whether program has access to
that page, while TLB valid bit indicates whether the translation is
“real” . When system first boots, no TLB translations are valid :)</li>
    </ul>
  </li>
  <li>What do when you context switch? Need to pull in new TLB for new
process</li>
  <li>You could just flush the TLB whenever you context switch, which sets
all the valid bits to 0</li>
  <li>This works, but you have to re-populate the TLB whenever you context
switch; if context switches happen often, you screwed</li>
  <li>
    <p>Hardware can add an <em>address space identifier</em> to the TLB, and each
process gets its own ASID. Then you only map VPNs that match your ASID
to the correct PFN. Cool</p>
  </li>
  <li>How do you decide which entry to replace when your TLB fills up?</li>
  <li>Could just do LRU</li>
  <li>Could also be random to avoid corner-case behaviors like “loop over n
    <ul>
      <li>1 pages with TLB of size n”, cause the one you want is always the
last thing removed so you have to fetch it</li>
    </ul>
  </li>
  <li>
    <p>They use an example of the MIPS TLB, cool</p>
  </li>
  <li>Summary: TLB on chip increases performance a lot :)</li>
  <li>But if you overrun the TLB (exceed TLB coverage) you may be fukked</li>
  <li>To get around this you can use larger page sizes; common in DBMS</li>
  <li>Cool</li>
</ul>

<h4 id="ostep-ch-20-paging-smaller-tables">OSTEP Ch 20: Paging: Smaller Tables</h4>

<ul>
  <li>Okay, page tables are very large (they have to map all possible
virtual page numbers to physical page numbers for every process!)</li>
  <li>Linear page tables suck</li>
  <li>
    <p>One solution is just having bigger page sizes, buuuut that leads to
internal fragmentation :/</p>
  </li>
  <li>What about combining paging and segmentation? <em>Hybrid</em>
</li>
  <li>Have one page table per logical segment (code, heap, stack)?</li>
  <li>Still use base and bounds for the segments</li>
  <li>Top two bits of virtual address refer to segment, then VPN number,
then offset number</li>
  <li>Keep a bounds register per segment :) because it’s separate from pure
page table by having different bounds at each segment</li>
  <li>
    <p>This hybrid approach is pretty good in general</p>
  </li>
  <li>Okay, how about an approach that doesn’t rely on segmentation, but
attacks same problem: how to get rid of all invalid regions in the
page table instead of keeping them all in memory? This is multi-level,
turns page table into a tree</li>
  <li>Chop up page table into page-sized units</li>
  <li>If entire page of page table entires is invalid don’t allocate that
page of the table</li>
  <li>Use a <em>page directory</em> to track whether each page of the page table is
valid or not</li>
  <li>Page directory has one entry per page table page. Each page directory
entry (PDE) has a valid bit and a page frame number (PFN). Is
pretty much like a page table entry</li>
  <li>Only allocates space in proportion to # of addresses you’re using</li>
  <li>Each portion of page table fits into one page, so you can manage the
memory easier :) just get next free page to find space more another
PDE</li>
  <li>Adds a level of indirection (cost) to improve space usage (benefit)</li>
  <li>First N bits of the VPN can be used as page directory index :)</li>
  <li>Use page directory index to find which page table you’re looking at,
then get your physical address</li>
  <li>Can use a multi-level page directory if the situation calls for it
:) pretty much just the same thing recursively</li>
  <li>We want the page directory to fit into one page of memory too!</li>
  <li>Cool that’s pretty much the process of translation here: use a page
table and have a TLB, nice</li>
  <li>
    <p>Teaser: page tables can be swapped to disk</p>
  </li>
  <li>Cool, so this is how real page tables are built, not just linear!</li>
  <li>All about dat time-space tradeoff; bigger page table = faster
servicing of TLB misses, but more space</li>
</ul>

<h4 id="ostep-ch-21-beyond-physical-memory-mechanisms">OSTEP Ch 21: Beyond Physical Memory: Mechanisms</h4>

<ul>
  <li>How can the OS make use of larger, slower device to transparently
provide the illusion of a large virtual address space?</li>
  <li>Why do this? For convenience + ease of use of the userland program</li>
  <li>Also multiprogramming: run multiple programs concurrently</li>
  <li>Reserve space on disk for moving pages back and forth: <em>swap space</em>
</li>
  <li>(since we swap pages out of and into memory from this space)</li>
  <li>
    <p>Need disk address of a page that has been swapped!</p>
  </li>
  <li>What do we need to add up above to support swapping?</li>
  <li>Add <em>present bit</em> to each page table entry. If 1, then page is in
physical memory. If 0, then page is swapped out</li>
  <li>If you access a page not in physical memory, it’s a page fault</li>
  <li>When this happens, hardware jumps to OS page fault handler</li>
  <li>Why does OS handle page faults instead of hardware? Accessing disk so
slow that the OS overhead isn’t the bottleneck. Also, hardware doesn’t
want to have to know about swap space/disk I/O, etc.</li>
  <li>OS may need to swap page back into physical memory. How sway</li>
  <li>
    <p>In the part of the PTE normally used for memory address, put a disk
address. Then OS just fetches from that address, puts it somewhere in
physical memory, flips valid bit, and replaces value in PTE with the
physical memory address</p>
  </li>
  <li>
    <p>Oh dear, how to swap in when memory is full? There’s a very good
page-replacement policy :)</p>
  </li>
  <li>Alright family, with page faults you obvz add to the flow control.
Some code examples here <img class="emoji" title=":zzz:" alt=":zzz:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a4.png" height="20" width="20">
</li>
  <li>When do you replace stuff? Want to do it proactively to help w/
performance</li>
  <li>Hgh watermark: minimum number of free pages you want available
whenever swap daemon runs</li>
  <li>Low watermark: when fewer free pages than this watermark, swap daemon
runs and evicts until you reach high water mark</li>
  <li>
    <p>Doing work in the background is good for efficiency :)</p>
  </li>
  <li>Cool, so we have a way to swap memory. Use a present bit, if page
isn’t present run the page-fault handler, this all happens
transparently to the process!</li>
</ul>

<h4 id="ostep-ch-22-beyond-physical-memory-policies">OSTEP Ch 22: Beyond Physical Memory: Policies</h4>

<ul>
  <li>
    <p>How does OS decide which page(s) to evict from memory? This decision
is made by the replacement policy of the system, which usually follows
some general principles but also includes certain tweaks to avoid
corner-case behavior</p>
  </li>
  <li>Let’s view main memory as a cache for virtual memory pages in system</li>
  <li>Thus, we want to minimize cache misses</li>
  <li>Average memory access time (AMAT) is the metric of choice. Computed
from hit rate, miss rate, cost of accessing memory, cost of accessing
disk</li>
  <li>“Optimal” replacement policy is to evict wpage that will be accessed
farthest in the future (obviously can’t do this ourselvs, but can
compare our implementations against this to benchmark)</li>
  <li>Quickly, cache miss types
    <ul>
      <li>Compulsory: because nothing in the cache yet</li>
      <li>Capacity: No space in the cache!</li>
      <li>Conflict: limits on where item can be in hardware cache, oops</li>
    </ul>
  </li>
  <li>
    <p>FIFO is first-in first-out (like a queue). Simple! 36% hit rate</p>
  </li>
  <li>
    <p>Random is simple as well…</p>
  </li>
  <li>LRU? Least Frequently Used? Let’s use historical data</li>
  <li>Stuff like frequency a page is accessed, recency of access</li>
  <li>These use <em>principle of locality</em>: stuff tends to get accessed
frequently and close by :)</li>
  <li>
    <p>Compare approaches by looking at different workloads. Some of these
workloads result in ugly corner cases :/</p>
  </li>
  <li>How do you implement a historical approach like LRU?</li>
  <li>Approximating easier than doing the real thing :)</li>
  <li>
    <p>Can use a <em>use bit</em> which is associated with a page table entry. Set
to 1 by processor if that page in memory is accessed. And
then have a “clock hand” that goes around all the page table entries
in a circle. When time to evict, if currently-pointed-at entry has a
use bit of 1, set it to 0, and track until you find one that’s 0. If
0, evict it</p>
  </li>
  <li>
    <p>Dirty pages suck to evict because you have to write them back to disk
first. Can use a <em>modified bit</em> similar to use bit that indicates
whether page has been modified, and try not to evict those</p>
  </li>
  <li>
    <p>Other policies: change up how you decide when to pull pages into
memory, or cluster writes to disk. All help with VM</p>
  </li>
  <li>
<em>Thrashing</em>: systems running don’t have stuff that fit into physical
memory, so you’re constantly swapping :(</li>
  <li>
    <p>Deal with it by choosing a subset of processes to get done with first?</p>
  </li>
  <li>Cool so those are policies for swapping</li>
</ul>

<h4 id="ostep-ch-23-the-vaxvms-virtual-memory-system">OSTEP Ch 23: The VAX/VMS virtual memory system</h4>

<ul>
  <li>Just read, no notes</li>
</ul>

<h4 id="lecture-3">Lecture</h4>

<ul>
  <li>
    <p><em>Cache pollution</em>: you keep doing disparate reads from memory, keep
having to flush and replace the cache, very bad</p>
  </li>
  <li>
    <p>Benefits of paging</p>
    <ul>
      <li>Code sharing for shared libraries</li>
      <li>Page caching. Cache code blocks that are frequently used by
processes!</li>
    </ul>
  </li>
</ul>

<h3 id="6---concurrency">6 - Concurrency</h3>

<h4 id="ostep-ch-26-concurrency-an-introduction">OSTEP Ch 26: Concurrency: An Introduction</h4>

<ul>
  <li>Multi-threaded processes! Need to manage memory accesses and whatnot</li>
  <li>
    <p>OS needs to deal with <em>locks</em> and <em>condition variables</em>, OS itself is
concurrent</p>
  </li>
  <li>
<em>Thread</em>: process that is multi-threaded has multiple instruction
pointers. They do share memory</li>
  <li>Each thread has its own set of registers to work from (need to do a
context switch when switching threads)</li>
  <li>Save execution state to a thread control block (TCB) for each thread</li>
  <li>Thread context switch does not need to switch address spaces :)</li>
  <li>
    <p>Multi-threaded address space has different spaces for each thread.
Called <em>thread-local</em> storage</p>
  </li>
  <li>Why use threads?</li>
  <li>Parallelism: have multiple running processes at the same time on
different processors</li>
  <li>
    <p>Avoid blocking due to I/O; threading enables overlap of I/O within a
single program (analogous to what multiprogramming does for processes)</p>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">pthread_create</code> to start a thread, <code class="language-plaintext highlighter-rouge">pthread_join</code> to wait for thread
to finish</li>
  <li>No guarantees on execution order of threads! This gets sad and
complicated :(</li>
  <li>Uh oh, shared data</li>
  <li>
<code class="language-plaintext highlighter-rouge">Pthread_create</code> (capital P) prints an error message if the thread
doesn’t create successfully</li>
  <li>Oh dear…accessing shared data (e.g. a shared counter) produces
unreliable, incorrect results</li>
  <li>Yikes, if you get a timer interrupt before persisting the saved
counter to the stack, you get screwed. So you can run the loop twice
but only increment the counter once :(</li>
  <li>This is what a <em>race condition</em> is; results in <em>indeterminate</em>
computation when multiple threads enter a critical section at the same
time</li>
  <li>Code where multiple threads are executing is a <em>critical section</em>.
Accesses a shared resource</li>
  <li>
    <p>What we want is <em>mutual exclusion</em>: if one thread is in the critical
section, it should be able to complete its work “as intended”</p>
  </li>
  <li>
<em>Atomic</em> operation: all updates happen at once time (as a unit, all or
none). Grouping of many actions into an atomic action is a
<em>transaction</em>
</li>
  <li>Start with <em>synchronization primitives</em> in the hardware</li>
  <li>Dis should make your head hurt</li>
  <li>
    <p>Another problem: one thread is dependent on another to complete action
before it continues</p>
  </li>
  <li>The OS is the first concurrent program, so that’s why this concern is
related to the OS
    <ul>
      <li>Example: two processes both want to write to a file at the same
time. How do you accomodate this? Untimely interrupt throws
everything off</li>
    </ul>
  </li>
</ul>

<h4 id="ostep-ch-27-interlude-thread-api">OSTEP Ch 27: Interlude: Thread API</h4>

<ul>
  <li>pthread_create(pthread_t * thread, const pthread_attr_t * attr, void *
(<em>start_routine)(void</em>), void *arg);</li>
  <li>Ezpz</li>
  <li>Pass pthread_t pointer to function to interact with thread</li>
  <li>Attr built with pthread_attr_init</li>
  <li>start_routine is a <em>function that takes one void pointer argument and
returns a void pointer</em>. Left side is return, right side is argument</li>
  <li>Void pointer means anything can be passed in/returned</li>
  <li>
<code class="language-plaintext highlighter-rouge">Pthread_join</code> waits for thread to complete</li>
  <li>Never return a pointer which refers to something allocated on the
thread’s call stack! It’ll be a seg fault next time you try to access
it</li>
  <li>Mutual exclusion comes by way of <em>locks</em>:
<code class="language-plaintext highlighter-rouge">pthread_mutex_lock(pthread_mutex_t *mutex)</code> and corresponding unlock
function</li>
  <li>Initialize a lock, lock before critical section, unlock after critical
section, nice. See man pages for more</li>
  <li>
<em>Condition variable</em>: use when you must signal between threads, if
one is waiting for another to finish something before it can
continue. Like <code class="language-plaintext highlighter-rouge">flag</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">pthread_cond_wait()</code> takes a cond_t and a lock mutex, puts calling
thread to sleep until it gets a <code class="language-plaintext highlighter-rouge">pthread_cond_signal</code> from another
thread that lets it continue. Use global variable <code class="language-plaintext highlighter-rouge">ready</code> for this
signaling
    <ul>
      <li>wait() takes a lock because it needs to release lock before going to
sleep. pthread_cond_wait() re-acquires lock before returning</li>
    </ul>
  </li>
  <li>Cool, the pthread library is just built in to C, hooray. Some
guidelines:
    <ul>
      <li>Keep it simple</li>
      <li>Minimize thread interactions</li>
      <li>Initialize locks and condition variables correctly</li>
      <li>Check return codes!!</li>
      <li>Be careful with how you pass arguments to, and return values from,
threads</li>
      <li>Each thread has its own stack!</li>
      <li>Use condition variables to signal between threads</li>
      <li>Use man pages</li>
      <li>zzz</li>
    </ul>
  </li>
  <li>Cool, get an idea for locks, condition variables, etc. by exploring
thread API in C</li>
</ul>

<h4 id="ostep-ch-28-locks">OSTEP Ch 28: Locks</h4>

<ul>
  <li>
<em>Lock variable</em> holds the state of the lock at any instant in time.
Lock can be <em>available</em> (unlocked, free) or <em>acquired</em> (locked, held)</li>
  <li>
    <p>One thread <em>owns</em> the lock at a time. If another thread calls lock(),
it will not return until the owner relinquishes control</p>
  </li>
  <li>
<em>mutex</em> provides mutual exclusion</li>
  <li>
    <p>Each lock is unique based on passed-in var, so you can have multiple
locks going on at a time</p>
  </li>
  <li>How do you build an efficient lock? Hardware? OS?</li>
  <li>Basic task: mutual exclusion</li>
  <li>
<em>Fairness</em>: each thread contending for lock get fair share at it?
    <ul>
      <li>Flip side: does any thread <em>starve</em> (never get a chance)?</li>
    </ul>
  </li>
  <li>
<em>Performance</em>: time overhead using a lock adds</li>
  <li>Naive: disable interrupts during lock
    <ul>
      <li>Many disadvantages</li>
      <li>Need to trust that locking thread is well-behaved</li>
      <li>Only works on single processor</li>
      <li>Interrupts (i.e. I/O done) can get lost if this runs for a long
time; leads to big problems</li>
      <li>Inefficient; masking interrupts is not easy</li>
    </ul>
  </li>
  <li>Okay, what about <em>test-and-set</em> instruction? Also called atomic
exchange</li>
  <li>Simplest: use a <code class="language-plaintext highlighter-rouge">flag</code> var that gets set to 1 while a thread is n
critical section. Unfortch, this still suffers from
concurrency/interrupt issue. Sad!</li>
  <li>
<code class="language-plaintext highlighter-rouge">test-and-set</code> instruction returns an old pointer value and updates
that pointer value, <em>atomically</em>
</li>
  <li>Implement lock: <em>test</em> old lock value to see if you can get the lock,
<em>set</em> new value at the same time</li>
  <li>Spin if you get an “old value” of 1, meaning that another thread had
the lock</li>
  <li>To frame concurrency problems, imagine you are a malicious scheduler
:)</li>
  <li>
    <p>Spin lock: correct, unfair, nonperformant on single CPU ‘cause you can
have a lot of spinning processes</p>
  </li>
  <li>Compare-and-swap? Pass in an expected and a new; set to new if actual
== expected</li>
  <li>
<code class="language-plaintext highlighter-rouge">lock</code> method just expects 0, sets 1. Cool</li>
  <li>
    <p>Similar to test and set now</p>
  </li>
  <li>Load-linked and store-conditional</li>
  <li>Acquire lock by spinning until loading flag is 0, doing a store conditional
to the lock address for the value 1 (make sure this returns 1! If it
doesn’t, someone else snuck in and got the lock</li>
  <li>Unlock by setting flag to 0, duh</li>
  <li>
    <p>Lauer’s Law: more code is worse. Less code is better</p>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">Fetch-and-add</code> atomically increments a value while returning old
value at the address</li>
  <li>
<code class="language-plaintext highlighter-rouge">Ticket lock</code>: to acquire lock, do an atomic fetch-and-add on the
ticket value. That’s the thread’s turn. Then, the <code class="language-plaintext highlighter-rouge">lock-&gt;turn</code>
determines which thread’s turn it is</li>
  <li>Unlock by incrementing the turn</li>
  <li>
    <p>This introduces a tad bit of fairness :)</p>
  </li>
  <li>How do you avoid spinning?</li>
  <li>
    <p>Just yield? Better, but can still do N context switches before
continuing your thread, where N is the number of threads trying to
acquire lock</p>
  </li>
  <li>Queue? Sleep, don’t spin</li>
  <li>
<code class="language-plaintext highlighter-rouge">park</code> puts calling thread to sleep, <code class="language-plaintext highlighter-rouge">unpark</code> wakes a thread</li>
  <li>Use a queue of lock waiters, unlock pops off the queue and hands off
control</li>
  <li>Spin locks suffer from <em>priority inversion</em>: higher priority thread
becomes the one that runs the most, but is not the one we want to run!</li>
  <li>All about addressing edge cases and stuff. setpark() just in case you
get interrupted during a park</li>
  <li>
    <p>Linux uses <em>futex</em>: each futex has a physical memory location,
futex_wait and futex_wake are the interface to get yourself onto the
queue and schedule the next thing on the queue, respectively</p>
  </li>
  <li>All kinds of different lock types! Details differ and stuff; this is
hard!</li>
</ul>

<h4 id="ostep-ch-29-lock-based-concurrent-data-structures">OSTEP Ch 29: Lock-based Concurrent Data Structures</h4>

<ul>
  <li>
    <p>How to add locks to data structures? Making them <em>thread-safe</em></p>
  </li>
  <li>Counter?</li>
  <li>Can just add a single lock, invoke whenever you do anything with the
data structure</li>
  <li>Scales poorly; we want <em>perfect scaling</em>, where a bunch of work is
done in parallel so we can scale without a performance cost</li>
  <li>
<em>Sloppy counting</em>: four local counters (one per CPU), one global
counter, each has its own lock. Increment local counter when your code
runs. Local values periodically transferred to global counter by
acquiring global lock and incrementing by local counter value; local
counter then set to 0</li>
  <li>Threshold S (for sloppiness) is how often the local-to-global transfer
occurs</li>
  <li>
    <p>Accuracy/performance tradeoff</p>
  </li>
  <li>List?</li>
  <li>Side note: exceptional control flow sucks</li>
  <li>Basically can just try to lock/unlock around inserts</li>
  <li>Side note: simple is usually fine!! If you don’t need to scale a ton</li>
  <li>General design tip: be wary of control flow changes that lead to
returns/exits that halt function execution. You probably left some
cleanup not done :/</li>
  <li>
    <p>Hand-over-hand: Add a lock per list node. But you don’t really see a
performance benefit. Maybe a hybrid? Where you grab a lock every few
nodes</p>
  </li>
  <li>Queues?</li>
  <li>Like everything, could add a big lock</li>
  <li>
    <p>Could maybe have separate locks for head and tail. <img class="emoji" title=":zzz:" alt=":zzz:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a4.png" height="20" width="20"></p>
  </li>
  <li>Hash table?</li>
  <li>
    <p>Use a lock per hash bucket! Each bucket is a linked list, so we can
lean on that implementation. Cool</p>
  </li>
  <li>
<em>Avoid premature optimization</em>!!!</li>
  <li>Okay, lists, queues, counters, hash tables, all good stuff that can be
concurrentized</li>
</ul>

<h4 id="ostep-ch-30-condition-variables">OSTEP Ch 30: Condition Variables</h4>

<ul>
  <li>How do you wait or a condition before proceeding in a thread?</li>
  <li>
<em>Condition variable</em>: explicit queue threads can put themselves on
when some state of execution is not as desired (<em>waiting</em> on the
condition); some other thread, when it changes state can then wake one
or more of these waiting threads and let them continue (by <em>signaling</em>
on the condition)</li>
  <li>
<code class="language-plaintext highlighter-rouge">pthread_cond_t</code> declares c as a condition variable. Has wait() and
signal() operations, yee</li>
  <li>
<em>Always hold the lock when calling signal or wait</em>, so that you can be
sure about your operations. Otherwise, you can get infinite loops and
such</li>
  <li>
    <p>Got it, parent join()ing on child does a wait(), child does a signal()
when it’s done. Need to also keep a <code class="language-plaintext highlighter-rouge">done == 0</code> state between parent
and child to ensure you don’t end up with infinitely running thread</p>
  </li>
  <li>
<em>Producer/consumer</em> or <em>Bounded buffer</em> problem</li>
  <li>Producers generate data items and place them in a buffer, consumers
grab items from buffer and consume them. E.g. a Unix pipe</li>
  <li>Producer only puts onto buffer when count is 0 (empty), Consumer reads
from buffer when count is 1</li>
  <li>
<em>Mesa semantics</em>: signaling a thread wakes it up (hint that world
state has changed), but no guarantee that the state is still as
desired when the woken thread runs!!
    <ul>
      <li>So if you have two consumers, and one sneaks in after other is
scheduled but not run yet, then the second one will raise assertion
when it tries to read from empty buffer</li>
    </ul>
  </li>
  <li>With Mesa, <em>always use while loops</em> so you know about state fo sho</li>
  <li>Need to handle multiple producers and consumers - more direct
signaling, not just one universal condition variable</li>
  <li>Producer waits on “empty”, signals on “fill”; consumer does the
opposite. So producer never wakes producer, consumer never wakes
consumer</li>
  <li>
<em>Correct</em> solution: allow buffer to fill up past 1 to be empty or
full, in buffer put() and get() set the fill_ptr and use_ptr to make
sure neither get up past the maximum you want, but otherwise, producer
can fill up a bit more and buffer can read out until <code class="language-plaintext highlighter-rouge">count</code> reaches
    <ol>
      <li>Cool</li>
    </ol>
  </li>
  <li>Condition checking in multi-threaded programs should always use
<code class="language-plaintext highlighter-rouge">while</code>
    <ul>
      <li>Avoids lots of bugs, such as when you get spuriously woken up. Make
sure to check on your state whenever waking up!</li>
    </ul>
  </li>
</ul>

<h4 id="ostep-ch-31-sempahores">OSTEP Ch 31: Sempahores</h4>

<ul>
  <li>Gg</li>
  <li>
<em>Semaphore</em>: synchronization primitive that can be used instead of
locks and condition variables</li>
  <li>Semaphore is an object wth an integer value that we can manipulate
with sem_wait() and sem_post()</li>
  <li>Must be initialized (e.g. to 1)</li>
  <li>sem_wait() decrements value of semaphore by one, waits if value is
negative</li>
  <li>sem_post() increments value of semaphore by one, if there are one or
more threads waiting, wake one by e.g. signaling on condition variable</li>
  <li>Value of semaphore == number of waiting threads if negative</li>
  <li>Wait before ciritcal section, post after critical section
    <ul>
      <li>Use as locks is simple: initialize to 1. This is a <em>binary
semaphore</em> because the only two states are held and unheld</li>
    </ul>
  </li>
  <li>Can be used as condition variables</li>
  <li>Parent calls sem_wait(), child calls sem_post() when it’s done. Cool</li>
  <li>Initialize to 0; will go to -1 when parent calls, wait, when child
calls post() it will go to 0 and wake up the parent</li>
  <li>
    <p>Even if child comes first, it will increment to 1, then parent sees
value &gt;= 0 when it runs so it just doesn’t wait in sem_wait()_</p>
  </li>
  <li>Bounded buffers?</li>
  <li>Can have separate full and empty semaphores, both used as condition
variables</li>
  <li>Issue with this: no mutual exclusion! Critical section of filling
buffer and incrementing index isn’t guarded. Add a binary sempahore to
use as lock :)</li>
  <li>Welp, deadlock: consumer runs, waits on full signal, producer runs,
waits on empty signal, they’re both stuck. Deadlock occurs when all
processes are stuck in a lock at the same time</li>
  <li>
    <p>Just change order of ops: check on binary semaphore <em>after</em> checking
on empty/full :)</p>
  </li>
  <li>Reader/writer lock: writes must have an exclusive lock, but reads can
be concurrent as long as there is no write happening</li>
  <li>Only a single writer can acquire the lock at a time</li>
  <li>
<em>First</em> reader acquires write lock too, so anyone who wants to write
has to wait for all readers to finish</li>
  <li>Okay, kinda yucky, readers can starve writers</li>
  <li>Implement semaphores with one condition variable and one lock</li>
  <li>
    <p>Tip: be careful when generalizing! There are subtle differences that
will bite you in the ass, and it’s rarely even needed</p>
  </li>
  <li>Semaphores are a nice primitive for concurrency, okay</li>
</ul>

<h4 id="ostep-ch-32-concurrency-bugs">OSTEP Ch 32: Concurrency Bugs</h4>

<ul>
  <li>Mostly skim, come back later</li>
  <li>What concurrency bugs to look out for? Can divide into non-deadlock
and deadlock</li>
  <li>Non-deadlock: <em>atomicity violation</em> and <em>order violation</em>
</li>
</ul>

<h4 id="lecture-4">Lecture</h4>

<ul>
  <li>
    <p>Realtime API/functionality allows you to run a block of code every N
ms, but you will get interrupted, must finish within N ms</p>
  </li>
  <li>
    <p>C convention to capitalize functions and wrap syscalls with uniform
error handling</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">ifndef</code> header guard in C programs makes sure you don’t include same
stuff twice</p>
  </li>
  <li>
    <p>When you use a lock, you claim that <em>everything else</em> outside of the
critical section can be interleaved with no issues</p>
  </li>
  <li>
    <p>Struct with two void pointers and array with two void pointers take up
the exact same amount of space</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">volatile</code> keyword tells the compiler “don’t reason about this value,
let it be”</p>
  </li>
</ul>

<h3 id="7---io-devices-hard-disks-and-flash-based-ssds-and-introduction-to-file-systems">7 - I/O devices, hard disks and flash based SSDs, and introduction to file systems</h3>

<h4 id="ostep-ch-36-io-devices">OSTEP Ch 36: I/O devices</h4>

<ul>
  <li>I/O is important. How should it be integrated into systems? What are
the general mechanisms? How can we make them efficient?</li>
  <li>CPU&lt;-&gt; memory by memory bus. Other devices by <em>I/O bus</em>
    <ul>
      <li>Could be PCI (graphics), or lower down like SCSI, SATA, USB</li>
      <li>It’s a hierarchy because shorter buses are more performant</li>
    </ul>
  </li>
  <li>Two parts of device: hardware interface, internal structure
    <ul>
      <li>
<em>Firmware</em>: software within hardware device</li>
    </ul>
  </li>
  <li>Say a device presents an interface with a status register, command
register, and data register (to pass data to device)</li>
  <li>First instinct is to have the OS poll the status register to see if
the device can receive data, but that is inefficient
    <ul>
      <li>When CPU involved with I/O directly, it’s programmed <em>I/O</em>
</li>
    </ul>
  </li>
  <li>Duh, interrupt. Overlap computation and I/O</li>
  <li>Interrupt has a context switching cost; if device is responsive, maybe
polling is ok
    <ul>
      <li>
<em>Coalesce</em> by holding onto interrupts for a little bit in hopes of
batching up multiple completed ops in one interrupt</li>
    </ul>
  </li>
  <li>We hate PIO because it’s costly, how do we eliminate?</li>
  <li>Use DMA: <em>Direct Memory Access</em>
</li>
  <li>DMA engine is a device that facilitates memory&lt;-&gt;device without CPU
help</li>
  <li>
    <p>OS just tells DMA engine address of data in memory that should be sent
to device, and DMA carries out request, then DMA controller raises
interrupt to tell OS that transfer is complete</p>
  </li>
  <li>How should hardware communicate with device?</li>
  <li>Can used privileged hardware I/O instructions (e.g. <code class="language-plaintext highlighter-rouge">in</code>/<code class="language-plaintext highlighter-rouge">out</code>,
specifying CPU registers with data and port)</li>
  <li>
    <p>Also can do <em>memory-mapped I/O</em>, where OS issues load/store to
specific address to communicate with ddevice</p>
  </li>
  <li>How to keep OS mostly device-neutral, hiding device interaction
details?</li>
  <li>
<em>Device driver</em> abstraction: software that tells OS how to interact
with device</li>
  <li>Device drivers make up 70% of Linux kernel! Also sometimes written by
scrubs, cause a lot of kernel crashes</li>
  <li>
    <p>OS code to interact with these devices just uses calls as you’d
expect, based on the driver interface</p>
  </li>
  <li>This chapter: interrupt and DMA for device efficiency, I/O
instructions and memory-mapped I/O for device communication</li>
</ul>

<h4 id="ostep-ch-37-hard-disk-drives">OSTEP Ch 37: Hard Disk Drives</h4>

<ul>
  <li>
    <p>How do hard disk drives store data? What is the interface? How is the
data laid out and accessed? How does disk scheduling improve
performance?</p>
  </li>
  <li>Modern disk</li>
  <li>Atomic 512-byte operations. <em>Torn write</em> when you want to do a larger
write but you die in the middle</li>
  <li>
<em>Platter</em>: one circular hard surface on which data is stored
persistently by inducing magnetic changes to it</li>
  <li>Disk can have multiple platters, each with two surfaces</li>
  <li>Platters bound together around <em>spindle</em>, connected to motor that
spins it around</li>
  <li>RPM: platter rotations per minute</li>
  <li>
<em>Track</em>: one of the many concentric circles on each surface</li>
  <li>
<em>Disk head</em>: reads/writes magnetic patterns</li>
  <li>
    <p><em>Disk arm</em>: moves across surface to position disk head over desired
track</p>
  </li>
  <li>Request processing</li>
  <li>
<em>Rotational delay</em>: how long it takes for desired sector to rotate
under disk head</li>
  <li>
<em>Seek time</em>: moving disk arm to the correct track :). Takes an
acceleration, coasting, deceleration, settling</li>
  <li>After rotation and seek, <em>transfer</em> can happen</li>
  <li>Disks have caches to hold some small amt of data, ez</li>
  <li>
    <p>Blah blah, doing some fraction simplification</p>
  </li>
  <li>I/O time?</li>
  <li>Size of transfer / time transfer took = rate of I/O</li>
  <li>Two scenarios: random workload, sequential workload</li>
  <li>
    <p>Order of magnitude difference. Use sequential whenever possible!</p>
  </li>
  <li>
<em>Disk scheduler</em> schedules disk operations</li>
  <li>Estimate seek and rotational delay, greedily pick the one that will
take the shortest amount of time :)</li>
  <li>SJF (shortest job first)</li>
  <li>This stuff is easier to estimate on disk than job scheduling</li>
  <li>Want to seek as few times as possible
    <ul>
      <li>Need to avoid starvation though, where you get a steady stream of
local requests and a request farther away never executes</li>
    </ul>
  </li>
  <li>SCAN goes from inner to outer tracks in order, ez?</li>
  <li>Doesn’t account for rotation :/</li>
  <li>End up at SPTF (shortest positioning time first) (also shortest access
time first)</li>
  <li>Estimate seek delay and rotation delay, go to the smallest one</li>
  <li>
    <p>Can age requests to help with starvation</p>
  </li>
  <li>Some issues</li>
  <li>Merging disparate I/O  from the same origin</li>
  <li>How long should system wait before issuing I/O request? Wait a lil bit
just in case you get a “better” request, or so that you can do
<em>vectored</em> interrupts</li>
  <li>
    <p>:thik</p>
  </li>
  <li>Good intro to hard disk drives :)</li>
  <li>Outdated a bit, but need the foundation</li>
</ul>

<h4 id="ostep-ch-38-redundant-arrays-of-inexpensive-disks-raids">OSTEP Ch 38: Redundant Arrays of Inexpensive Disks (RAIDs)</h4>

<ul>
  <li>Large, fast, reliable disk?</li>
  <li>RAID is super complicated</li>
  <li>Advantages:
    <ul>
      <li>Performance. Parallel access super fast</li>
      <li>Capacity. Can store lots</li>
      <li>Redundancy. Can tolerate loss :)</li>
    </ul>
  </li>
  <li>
    <p><em>Transparency</em> is a darn good principle - can you add functionality
without imposing more of a burden on your user/client?</p>
  </li>
  <li>System issues <em>logical</em> I/O to RAID, which does some calculation and
then does the rgiht corresponding <em>physical</em> I/O</li>
  <li>
    <p>RAID can be thought of as a specialized computer system running
special disk access software</p>
  </li>
  <li>Fault model critical to understanding these things</li>
  <li>
    <p><em>Fail-stop</em>: disk can be working or failed. Working = do stuff, failed
= assume permanently lost</p>
  </li>
  <li>Properties:</li>
  <li>Capacity (total capacity - redundant copies)</li>
  <li>Reliability</li>
  <li>
    <p>Performance (workload-dependent :/)</p>
  </li>
  <li>Patterson/Gibson/Katz at Berkeley came up with important RAID designs</li>
  <li>Keep in mind <em>mapping problem</em>: how to translate logical I/O request
to exact disk and offset in RAID</li>
  <li>Level 0: <em>striping</em>: stripes, 0 1 2 3, 4 5 6 7, etc. across disks</li>
  <li>0 1 2 3 is a stripe there</li>
  <li>Chunk size is how much of one stripe is on one disk before moving to
next one</li>
  <li>Level 0: perfect capacity, wacko reliability (any failure leads to
loss), performance is great since all disks are used :)</li>
  <li>Some metrics: single-request latency (how long one request takes),
steady-state throughput (total bandwidth of many concurrent requests;
how much can you handle?)</li>
  <li>Same random/sequential access measurements as before</li>
  <li>Raid-0 has same single request latency as one disk :)</li>
  <li>
    <p>Okay</p>
  </li>
  <li>Level 1: Mirroring, make one extra copy of each block in system, on a
separate disk</li>
  <li>Half capacity</li>
  <li>Decent reliability (can handle one disk failure, cool)</li>
  <li>Reaad latency is good, write latency is up to double since it has to write
to two places. IRL writes are parallelized, but still have to wait for
longer one</li>
  <li>
<em>Consistent-update problem</em> in RAIDs: if crash in between write 1 and
write 2 of same data, you screwed.</li>
  <li>To fix, use a write-ahead log that keeps track of operations you were
about to do if you lose power at a bad time. When you recover, just
apply those transactions</li>
  <li>
    <p>Most of these performance metrics are intuitive :)</p>
  </li>
  <li>Level 4: Disk with parity!</li>
  <li>You have a parity disk that has the xor of the stripe from all the
other disks</li>
  <li>If one disk goes down, you can recover what it had by returning to the
same xor values</li>
  <li>Better capacity than level 1</li>
  <li>But can only tolerate one disk failure</li>
  <li>
    <p>Boring performance analysis, zzz</p>
  </li>
  <li>Level 5: Rotating parity</li>
  <li>Parity block for each stripe is now rotated across the disks, in order
to remove the parity-disk bottleneck from RAID-4</li>
  <li>Better random read performance</li>
  <li>Most of analysis is the same :)</li>
  <li>This is dominant now</li>
  <li>
    <p>Cool, table of performances here</p>
  </li>
  <li>Transform independent disks into large, more capacious, more reliable
single entity <em>transparently</em> :) :)</li>
  <li>More of art than science to pick correct RAID layout and such</li>
  <li>Cool</li>
</ul>

<h4 id="ostep-appendix-flash-based-ssds">OSTEP Appendix: Flash-based SSDs</h4>

<ul>
  <li>Pretty applicable. Solid-state storage, with no moving parts</li>
  <li>Retains info during power loss</li>
  <li>Write a <em>flash page</em> by erasing a <em>flash block</em> first; writing same
page too often will <em>wear it out</em>
</li>
  <li>How sway</li>
  <li>Hierarchy: block-&gt;page-&gt;content</li>
  <li>Operations: read page, erase block (must happen before writing),
program page</li>
  <li>States: invalid (start), erased (after erase), valid (after
programmed)</li>
  <li>
    <p>Okay. Read is easy, write is harder. Must copy data we care about
somewhere else first, and also have to deal wit wear out</p>
  </li>
  <li>Evaluate flash</li>
  <li>Latency is p good; reading is superfast, programming is okay, erasing
takes a ton of time</li>
  <li>More reliable than hard disk since no moving parts</li>
  <li>
    <p>Can have disturbances when you write to one page and bits get flipped
elsewhere. dang</p>
  </li>
  <li>How to use this as storage?</li>
  <li>Flash chips for persistent storage :)</li>
  <li>Must provide standard block interface on top of your flash chips</li>
  <li>
<em>Flash translation layer</em> satisfies client reads and writes and turns
them into flash operations if need be</li>
  <li>Performance achieved by parallel flash chip usage
    <ul>
      <li>Reduce write amplification (total write traffic from flash chips to
FTL / total write traffic)</li>
    </ul>
  </li>
  <li>Direct mapping of logical page to physical page sucks. Tons of erases
and writes. Wear out of hot paths is RIP</li>
  <li>Log structure is better</li>
  <li>On a logical write, write appended to next free spot in
currently-being-written-to block (this is logging)</li>
  <li>Mapping table stores physical address of each logical block</li>
  <li>Okay, example. Think I get it. Compact stuff so you can avoid wear
out, do as few erases as possible, avoid program disturbance</li>
  <li>
    <p>Big improvement :)</p>
  </li>
  <li>Garbage collection</li>
  <li>Get garbage when you have old versions of data around the drive (stuff
that’s been overwritten elsewhere)</li>
  <li>Find a block that contains one or more garbage pages</li>
  <li>Read live pages from that block</li>
  <li>Write those pages to the log</li>
  <li>Reclaim entire block for use in writing</li>
  <li>
    <p>Each block needs metadata on whether or not data in page is live</p>
  </li>
  <li>Mapping table gets big; page-based mapping is impractical. Maybe try
with block-level FTL? Akin to having bigger page sizes in virtual
memory :)</li>
  <li>Bleh, lose granularity so you need to do a copy and stuff on every
write</li>
  <li>Issue when writes are smaller than physical block size. Sigh</li>
  <li>
<em>Hybrid mapping</em>: reserve specific blocks for writing, keep a page
mapping for those. Keep a block mapping for everything else</li>
  <li>Page mapping = log table, block mapping = data table</li>
  <li>Look at log table adn then page table when searching for a logical
block</li>
  <li>Okay</li>
  <li>
    <p>All this time, make sure to keep wear leveling in account</p>
  </li>
  <li>Is everything flash chips? Where do you keep the other stuff
persistently?
    <ul>
      <li>Flash chips for persistent storage, cool. Tricky part is just
knowing that stuff can get moved around, got it</li>
    </ul>
  </li>
  <li>More performant, more expensive than hard disk drives</li>
  <li>State of the art is a doozy</li>
</ul>

<h4 id="lecture-5">Lecture</h4>

<ul>
  <li>Device drivers suck :( hardest part of kernel development</li>
  <li>
<em>Livelock</em> is opposite of deadlock: so many interrupts that you can’t
tell the machine to stop accepting them!!</li>
  <li>DMA: devices don’t map to virtual addresses, they map to physical
memory when the kernel boots up</li>
  <li>
<em>inode</em>: struct that represents a file or directory object</li>
  <li>time updated, time created, type (file/dir)</li>
  <li>blocks[64], where the blocks to get this file at are from?</li>
</ul>

<h3 id="8---file-systems">8 - File Systems</h3>

<h4 id="ostep-ch-39-file-systems-introduction">OSTEP Ch 39: File Systems: Introduction</h4>

<ul>
  <li>
    <p>How should the OS manage a persistent device? What are the APIs?</p>
  </li>
  <li>
<em>File</em>: linear array of bytes, each of which can be read or written
    <ul>
      <li>Has a low-level name (number of some kind): its <em>inode number</em>
</li>
      <li>FS doesn’t know what type of file, it just stores the bytes</li>
    </ul>
  </li>
  <li>
<em>Directory</em>: contents are a list of &lt;user name, inode name&gt; pairs
    <ul>
      <li>Has an inode number of its own</li>
      <li>Each entry refers to a file or other dir</li>
      <li>Results in directory tree</li>
    </ul>
  </li>
  <li>
<em>Root directory</em> is start of the hierarchy on Unix
    <ul>
      <li>Separator like “/” names subdirectories until desired subdir or file
is named</li>
    </ul>
  </li>
  <li>
    <p>Naming is hard!! Everything on Unix can be named on file system, which
is a handy abstraction</p>
  </li>
  <li>File system ops</li>
  <li>
<code class="language-plaintext highlighter-rouge">open()</code> to create or open file. Returns a <em>file descriptor</em>
    <ul>
      <li>A private per-process integer used to access files. It’s an opaque
handle giving you power to perform certain operations</li>
      <li>0 is stdin, 1 is stdout, 2 is stderr, so the first open() gives you
3</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">strace</code> handy! Checks system calls made by program while it runs</li>
  <li>
<code class="language-plaintext highlighter-rouge">lseek</code> to move the <em>file offset</em> to a specific place. Doesn’t
actually move disk head! Just changes value in software</li>
  <li>
<code class="language-plaintext highlighter-rouge">fsync</code> persists writes that are in-flight, <img class="emoji" title=":zzz:" alt=":zzz:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a4.png" height="20" width="20">
</li>
  <li>
<code class="language-plaintext highlighter-rouge">rename</code> is <em>atomic</em> op to rename a file</li>
  <li>
<code class="language-plaintext highlighter-rouge">stat &lt;file&gt;</code> gives you info about file</li>
  <li>
<code class="language-plaintext highlighter-rouge">unlink</code> to remove a file. Why “unlink”?
    <ul>
      <li>Opposite of <code class="language-plaintext highlighter-rouge">link</code> which creates another way to refer to same file.
Remove yourself from list of links from that file == “delete”. File
deleted when <em>reference count</em> becomes 0, sigh</li>
    </ul>
  </li>
  <li>
<em>Symbolic link</em> different from hard link
    <ul>
      <li>Has pathname of linked-to file as data of symlink file</li>
      <li>If original file removed, you have a <em>dangling reference</em>, oops</li>
    </ul>
  </li>
  <li>
    <p>Finally, <code class="language-plaintext highlighter-rouge">mount</code> takes existing dir as a target <em>mount point</em> and
pastes a new file system into directory tree at that point. Many diff
file systems on your system!</p>
  </li>
  <li>Then directory stuff</li>
</ul>

<h4 id="ostep-ch-40-file-systems-implementation">OSTEP Ch 40: File Systems: Implementation</h4>

<ul>
  <li>
    <p>How can we build a simple file system? What structures are needed on
disk? What do they need to track? How are they accessed?</p>
  </li>
  <li>Mental models of filesystems</li>
  <li>
<em>Data structures</em>: what types of on-disk structures are utilized by
the file system to organize data and metadata?</li>
  <li>
    <p><em>Access methods</em>: how does it map calls made by process
(open/read/write) onto its structures? What data structures do you
read/write from to make this stuff happen?</p>
  </li>
  <li>Start FS by dividing disk into 4kb blocks</li>
  <li>
<em>Data region</em> is blocks for actual stuff, majority of our space should
be for this</li>
  <li>
<em>Inode table</em> stores all of our inodes
    <ul>
      <li>Inode table size limits # of files you can have on disk!</li>
      <li>Inode short for <em>index node</em>
</li>
    </ul>
  </li>
  <li>Need a <em>free list</em> or <em>bitmap</em> to track free space in inode table/data
region
    <ul>
      <li>Bitmap: each bit is 1 or 0 to indicate whether block at that index
is free or nah</li>
    </ul>
  </li>
  <li>Finally, <em>superblock</em> has metadata about FS; # of inodes, # of data
blocks, where inode table lives, etc.
    <ul>
      <li>OS reads this when first mounting a filesystem</li>
    </ul>
  </li>
  <li>Index into inode table to find your inode, then you can jump to place
on disk where your file lives!
    <ul>
      <li>Bunch of data on the inode - permissions, size, owner, etc.</li>
    </ul>
  </li>
  <li>Can have <em>direct pointer</em> from inode to location on disk</li>
  <li>Say you have 12 direct pointers and run out - keep one <em>indirect
pointer</em> that points to a separate block that contains many more
pointers which point to user data. Indirection hooray
    <ul>
      <li>Can also do an extent (pointer+length), but whatever</li>
      <li>Also you can nest all of these pointers, of course…</li>
    </ul>
  </li>
  <li>
    <p>Why this design? Most files are small, so can be covered by the direct
pointers :)</p>
  </li>
  <li>Directories! Store info about each child file/dir
    <ul>
      <li>inode number, record length (length of name + padding), string length
(actual length of name), and name itself</li>
    </ul>
  </li>
  <li>
    <p>Stored in the same way as files! Inode has direct pointer to space on
block for dir</p>
  </li>
  <li>
    <p>Free space management is a doozy as usual. Our bitmap works for simple
filesystem though. Could get fancy with B tree</p>
  </li>
  <li>Now let’s consider access methods</li>
  <li>Data structures to be touched for reading /foo/bar:
    <ul>
      <li>Data bitmap</li>
      <li>inode bitmap</li>
      <li>Root inode</li>
      <li>Foo inode</li>
      <li>Bar inode</li>
      <li>Root data</li>
      <li>foo data</li>
      <li>bar data[0]</li>
      <li>bar data[1]</li>
      <li>bar data[2]</li>
    </ul>
  </li>
  <li>File read timeline is straightforward</li>
  <li>Write to the /foo/bar inode with the last access time whenever you
continue your read</li>
  <li>
    <p>Allocation structures only accessed for allocations (writes, creates),
not reads</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>As you can tell, this is a LOT of I/O :</td>
          <td>how fix?</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Keep a fixed-size LRU cache in memory for frequently accessed files
and such</li>
  <li>Can use <em>write buffering</em> to batch up writes close together, wait to
see if maybe file gets deleted so no write needed, etc.</li>
  <li>
<em>Durability/Performance tradeoff</em>: think of the types of applications
you are servicing, and make the pragmatic decision. E.g. databases
need durability</li>
  <li>Cool beans, this is filesystems!</li>
</ul>

<h4 id="ostep-ch-41-locality-and-the-fast-file-system">OSTEP Ch 41: Locality and the Fast File System</h4>

<ul>
  <li>
    <p>First file system as outlined last chapter was slow, got slower as
disk filled up, treated disk like it was RAM (BAD!), got fragmented :/</p>
  </li>
  <li>
    <p>How do you organize file system data structures to improve
performance? What type of allocation policies should live on top of
those data structures? How do we make the file system “disk aware”?</p>
  </li>
  <li>First pass: FFS (Fast File System)</li>
  <li>Keep same interface, but with different implementation</li>
  <li>Divide disk into <em>block groups</em> or <em>cylinder groups</em>, basically for
spacial locality of data on disk</li>
  <li>In each group, keep a supernode, inode bitmap, data bitmap, inodes,
and data!</li>
  <li>Aside: file creation is a lot of work! Need to update data bitmap,
data block, directory, inode of directory, whew</li>
  <li>
<em>Keep related stuff together</em>
    <ul>
      <li>dirs: balance across groups by finding a group with low # of dirs</li>
      <li>files: allocate data blocks of file in same group as its inode, put
all files in same directory in the same group as their directory.
Cool</li>
    </ul>
  </li>
  <li>These aren’t research heuristics :) they are simply based on <em>common
sense</em>: files in same dir are often accessed together</li>
  <li>
<em>Large file exception</em>: instead of filling all space in one group with
large file, spread across multiple groups.
    <ul>
      <li>Performance hit :/ but try to make the cost of moving b/t groups
<em>amortized</em>; if you spend most of your time transferring data from
disk (since it’s a large file), cost of seeking is amortized</li>
    </ul>
  </li>
  <li>For small files: use a set of 512B sub-blocks, allocate those, when
you allocate a 4kb page worth of sub-blocks, copy to a new location
and free the sub-blocks :). Can also buffer small file writes. Cool</li>
  <li>
    <p>Lesson for all CS: FFS was <em>usable</em>, included several nicety features
that can’t necessarily be researched (symlinks, long file names,
atomic rename), which drove adoption</p>
  </li>
  <li>FFS was a big step. Major takeaway: <em>treat the disk like it’s a disk</em>
</li>
</ul>

<h4 id="ostep-ch-42-crash-consistency-fsck-and-journaling">OSTEP Ch 42: Crash Consistency: FSCK and Journaling</h4>

<ul>
  <li>
    <p>System may crash/lose power between writes, leaving on-disk state
partially updated. Given that crashes occur at arbitrary points in
time, how do we ensure that the file system keeps the on-disk image in
a reasonable state?</p>
  </li>
  <li>Take one write. Must write to the inode, the bitmap, and the data
block. At minimum</li>
  <li>When can you crash?
    <ul>
      <li>Just data block written todisk</li>
      <li>Just inode written to disk</li>
      <li>Just updated bitmap is written to disk</li>
      <li>Any combination of two of these</li>
      <li>Sigh</li>
    </ul>
  </li>
  <li>
    <p><em>Crash consistency problem</em>: How can you make this write <em>atomic</em>,
even though disk only commits one write at a time?</p>
  </li>
  <li>Naive/early: <code class="language-plaintext highlighter-rouge">fsck</code> (File system checker). Run before file system is
mounted/made accessible
    <ul>
      <li>Sanity check superblock. If bad, maybe use a copy</li>
      <li>Check free blocks - if inodes/bitmaps inconsistent, trust the inodes</li>
      <li>Check inode state - valid enums, etc. Suspect inodes cleared</li>
      <li>Check inode link count by building its own link count map and
comparing to values in inodes</li>
      <li>Check duplicate inodes pointing to same block. Can copy or clear</li>
      <li>zzzzz</li>
    </ul>
  </li>
  <li>
    <p>Doesn’t scale well. Also inefficient - basically,
search-the-entire-house-for-keys instead of doing a smarter search</p>
  </li>
  <li>
<em>Write-ahead logging</em>, called <em>journaling</em> in file systems. We’ve seem
this before :)</li>
  <li>When updating disk, before overwriting structures, write a log entry
describing what you’re about to do</li>
  <li>If crash during update, go back and look at node and see exactly what
needs fixing and how to fix it</li>
  <li>Tradeoff: adds work during updates to reduce work during recoveries</li>
  <li>Each entry is associated with a <em>transaction identifier</em>. Naive
approach is to have each log entry keep the exact contents of blocks
(<em>physical logging</em>) that you are updating</li>
  <li>
<em>Checkpointing</em> is looking at the log and applying the actions. If all
actions complete, you’ve successfully checkpointed and are basically
done. So the two steps are <em>journal write</em> and <em>checkpoint</em>
</li>
  <li>What if you crash when writing to journal?
    <ul>
      <li>First, write everything except TxnEnd block to journal</li>
      <li>When the writes complete, write the TxnE block</li>
      <li>If you see that there’s no TxnE block, you know that the journal
write was incomplete</li>
      <li>This <em>journal commit</em> step is b/t journal write and checkpoint</li>
    </ul>
  </li>
  <li>Simple recovery: <em>redo logging</em>, just replay all the things in the
log. A few redundant writes don’t hurt, since you don’t do this often
(hopefully)</li>
  <li>
    <p>Buffering helps avoid excessive write traffic again - if you have two
close-by file creates, can batch them into one transaction :)</p>
  </li>
  <li>What do about journal size?</li>
  <li>Similar to a <em>ring buffer</em>, make it circular by just keeping a pointer
to the first non-garbage transaction :)</li>
  <li>After checkpointing, mark the transaction free in the journal by
updating the journal superblock</li>
  <li>What about cost of <em>data journaling</em>, basically dual writing
everything?
    <ul>
      <li>Try <em>metdata journaling</em> instead (this is more common!)</li>
      <li>Journal only keeps inode and block info, do the <em>data write</em> before
you do anything to the journal. Ensure pointer never points to
garbage</li>
    </ul>
  </li>
  <li>
    <p>Tricky corner case: if you reuse a block in your log that used to be a
dir but is now a file, your metadata log contains info about writing
dir info but not user data (since dir info considered metadat). If you
add a <em>revoke</em> entry to your log, recovery will look for all entries
that have been revoked and not apply them</p>
  </li>
  <li>
    <p>Lots of other approaches to this problem! Cool beans ig</p>
  </li>
  <li>If you don’t wanna write “COMMIT”, you can write a checksum of the
journal entry contents at the beginning and end of the entry instead.
Saves one step</li>
</ul>

<h4 id="ostep-ch-43-log-structured-file-systems">OSTEP Ch 43: Log-structured File Systems</h4>

<ul>
  <li>New file system. Why?
    <ul>
      <li>System memory growing, more space for cache</li>
      <li>Large gap between random and sequential I/O is felt more</li>
      <li>Existing FS perform poorly on common workloads; simple write takes a
lot of I/O</li>
      <li>FS aren’t RAID-aware; to do small amount of parity writing, yucky
amount of I/O takes place</li>
    </ul>
  </li>
  <li>
<em>Log-structured file system</em>: buffer all updates in an in-memory
<em>segment</em>; when segment is full, it is written to disk in one long,
sequential transfer to unused part of disk
    <ul>
      <li>Always to a free location</li>
      <li>Large segment = efficient disk use</li>
    </ul>
  </li>
  <li>How can a file system transofrm all writes into sequential writes?
Can’t do this for reads since you don’t decide, but can control for
writes!</li>
  <li>Basically, instead of writing inodes and data in separate places,
write it all in a row</li>
  <li>
    <p><em>Write buffering</em>: LFS keeps track of updates in memory before writing
to disk. When it has enough stuff, it writes the entire segment at
once, which is efficient</p>
  </li>
  <li>How much to buffer?
    <ul>
      <li>Every time you write, you pay a fixed overhead of positioning cost.
Need to make write big enough to amortize that cost. Wait too long
though, and you may be screwed</li>
    </ul>
  </li>
  <li>How do you find an inode? Use <em>inode map</em> (imap). Take inode number as
input and produces disk address of most recent version of inode</li>
  <li>Imap is also written sequentially!! How tho <strong>ask</strong>
</li>
  <li>Now how to find these disparate imap locations? With the disk’s
(fixed) <em>checkpoint region</em>
    <ul>
      <li>Pointers to latest pieces of inode map so you can start to find your
way around</li>
      <li>Only update this every 30 seconds or so</li>
      <li>Can be read into memory first, following checkpoint region all the
way down</li>
    </ul>
  </li>
  <li>Directory structure basically identical to other FS - &lt;name, inode
number&gt; pairings</li>
  <li>To access /tiger/foo, first look up location of tiger dir in imap,
readthe dir inode, which gives you location of dir data, which gives
you mapping of name to inode number, then look up the inode for foo,
once you find it you can get to foo. Cool</li>
  <li>
<em>Recursive update problem</em>: happens if FS doesn’t update in place, but
rather moves to a new spot
    <ul>
      <li>When inode updated, location on disk changes, which changes the dir
pointing to that file, which causes changes all the way up the tree</li>
      <li>To solve: rather than updating the dir itself, update the imap
structure to point to new location:)</li>
    </ul>
  </li>
  <li>Garbage collection? Old versions are hanging around because you only
write new versions of files
    <ul>
      <li>Done by <em>compacting</em> M existing segments (with mixed live and dead
data) into a smaller number N new segments with just the live data.
Can then clean all of M</li>
    </ul>
  </li>
  <li>
    <p>Use <em>segment summary block</em> at head of segment describing inode number
and offset of each data block in the segment. Look up inode of file
from imap, if the offset in your segment matches the data that is in
the imap, you know your data is live</p>
  </li>
  <li>Recovery?</li>
  <li>Crash during CR write: a) keep two copies of CR, one at each end of
file system, for atomicity. b) When writing to CR, write a header with
timestamp, then update, then a footer with timestamp - if header
present but not footer, you don’t have a complete picture</li>
  <li>
    <p>Lose stuff since last checkpoint though? Not quite. Try a <em>roll
forward</em> using log stored in the CR, try to apply any of the changes
you can</p>
  </li>
  <li>This <em>copy-on-write</em> approach enables highly efficient writing :)</li>
</ul>

<h4 id="ostep-ch-44-data-integrity-and-protection">OSTEP Ch 44: Data Integrity and Protection</h4>

<ul>
  <li>Basically, new storage mechanisms open up new types of faults, and we
need to come up with new solutions</li>
</ul>

<h4 id="xv6-ch-6">xv6 ch 6:</h4>

<h4 id="lecture-6">Lecture:</h4>

<ul>
  <li>Really great problem solving here by journaling, folks</li>
  <li>Everything hinges on <em>commit</em> keyword</li>
</ul>

<h3 id="9---review">9 - Review</h3>

<h4 id="lecture-7">Lecture</h4>

<h5 id="gary-bernhardt---the-birth-and-death-of-javascript">Gary Bernhardt - The Birth and Death of JavaScript</h5>

<ul>
  <li>https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript</li>
  <li>JavaScript sucked in the beginning</li>
  <li>Yikes, at first you get a ton of weeeeeeeird language stuff</li>
  <li>Then ASM.js which allows you to get native integer adds and stuff, if
you type annotate your code (i.e. x|0 (bitwise or) casts to integer)
finally in JavaScript
    <ul>
      <li>An extraordinarily optimizable, low-level subset of JavaScript. NOt
bad. Helps run games and terminals in browser</li>
      <li>C programs can compile to asmjs</li>
    </ul>
  </li>
  <li>Digression: how computers actually work</li>
  <li>Virtual Memory: ez</li>
  <li>Protection: Ring 3 protection (userland, can’t manipulate physical
memory and stuff)
    <ul>
      <li>Kernel runs in Ring 0, which has these permissions</li>
    </ul>
  </li>
  <li>Function calls - jump around in instruction pointer</li>
  <li>Finally, syscalls
    <ul>
      <li>Userland pushes registers, firest interrupt</li>
      <li>Kernel traps interrupt, switch to ring 0, switch Virtual Memory
Table, jump to syscall code
        <ul>
          <li>Context switch is overhead</li>
          <li>If you compile to JS, you can have your VM in the kernel and no
context switches plz</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ASM goes through VM still -20%; on metal, you get 20% performance gain</li>
  <li>His definition of metal: Kernel + asm.js + DOM at native speed</li>
</ul>

<h5 id="booting-up">Booting Up</h5>

<ul>
  <li>Press power button, current flows, detect RAM
    <ul>
      <li>No RAM? Gotta do some beeps, cause can’t display anything without
RAM</li>
    </ul>
  </li>
  <li>Load program from BIOS into RAM and run (512kb)
    <ul>
      <li>Basic Input/Output Sytem (Read-only memory)</li>
      <li>Checks for connected devices</li>
      <li>Comes with computer</li>
    </ul>
  </li>
  <li>Look for boot sector - this is on desk
    <ul>
      <li>
<em>Contains</em> boot loader (512 bytes)</li>
      <li>Must find one on one of the disks</li>
      <li>Pick a kernel to boot up into</li>
    </ul>
  </li>
  <li>Kernel is loaded
    <ul>
      <li>Climb to long mode (go up from 16 bit instructions to 64)</li>
      <li>Configure CPU
        <ul>
          <li>Interrupt handlers - syscall, etc.</li>
          <li>How often to do a timer interrupt. Can have multiple!</li>
          <li>Configure page table</li>
          <li>Configure protection rings</li>
        </ul>
      </li>
      <li>Configure other hardware</li>
      <li>Load kernel threads into schedule table</li>
      <li>Load process 0</li>
      <li>Execute process 0
        <ul>
          <li>
            <h2 id="start-daemon-processes-will-run-forever-in-dependency-order">Start daemon processes (will run forever) in dependency order</h2>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Check out Mozilla OS Guide?</li>
</ul>

<h3 id="extras">Extras</h3>

<ul>
  <li>Inode has id, type, blocks, permissions, times (modified, created),
nlinks, size</li>
</ul>

</article>













        </div>
      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      © 2018
    </small>
  </div>
</footer>
<!-- AnchorJS -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.0.0/anchor.min.js"></script>
<script>
    anchors.options.visible = 'always';
    anchors.add('article h2, article h3:not(.no-anchor), article h4:not(.no-anchor), article h5:not(.no-anchor), article h6');
</script>


  <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.20.1/moment.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.css">
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch-theme-algolia.min.css">

<script>
const search = instantsearch({
  appId: 'Q1JJ6NM1UC',
  indexName: 'tigerthinks',
  apiKey: '5a99d8bb6e9d1a03163a5eb0d67496e0',
  searchFunction: function(helper) {
    const searchHits = document.getElementById("search-hits")
    const content = document.getElementById("content")

    if (helper.state.query === '') {
      searchHits.style.display = "none";
      content.style.display = "block";
      return;
    }

    helper.search();
    searchHits.style.display = "block";
    content.style.display = "none";
  },
  searchParameters: {
    hitsPerPage: 10
  }
});

const hitTemplate = function(hit) {
  let date = '';
  if (hit.date) {
    date = moment.unix(hit.date).format('MMM D, YYYY');
  }

  let url = `${hit.url}#${hit.anchor}`;

  const title = hit._highlightResult.title.value;

  let breadcrumbs = '';
  if (hit._highlightResult.headings) {
    breadcrumbs = hit._highlightResult.headings.map(match => {
      return `<span class="post-breadcrumb">${match.value}</span>`
    }).join(' > ')
  }

  const content = hit._highlightResult.html.value;

  return `
    <div class="post-item">
      <span class="post-meta small">${date}</span>
      <a class="post-link" href="${url}"><h2 class="search-hit-title post-title">${title}</h2></a>

      
      <a href="${url}" class="post-breadcrumbs"><h5>${breadcrumbs}</h5></a>
      

      <div class="post-snippet">${content}</div>
    </div>`;
}

// Adding searchbar and results widgets
search.addWidget(
  instantsearch.widgets.searchBox({
    container: '#search-searchbar',
    placeholder: '',
    poweredBy: true,
    autofocus: false,
    cssClasses: {
      root: 'tigerthinks-searchbar'
    }
  })
);


search.addWidget(
  instantsearch.widgets.hits({
    container: '#search-hits',
    templates: {
      item: hitTemplate
    }
  })
);

// Starting the search
search.start();
</script>

<style>
.ais-search-box {
  max-width: 100%;
}

.post-item {
  padding-top: 20px;
  padding-bottom: 20px;
  border-bottom: thin solid #f3f3f3;
}

.post-link .ais-Highlight {
  color: #0076df;
  font-style: normal;
}

.post-breadcrumbs {
  font-style: normal;
  display: block;
  padding-bottom: 10px;
  background-image: none !important;
  color: #333 !important;
}

.post-breadcrumb {
  font-style: normal;
  font-size: 18px;
  color: #333;
}

.post-breadcrumb .ais-Highlight {
  font-weight: bold;
  font-style: normal;
  color: #0076df;
}

.post-snippet .ais-Highlight {
  color: #0076df;
  font-style: normal;
  font-weight: bold;
}

.post-snippet img {
  display: none;
}
</style>

</body>
</html>
