<!DOCTYPE html>
<html>
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Computer Architecture and the Hardware/Software Interface &#8211; Tiger Shen</title>
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Lecture notes from Bradfield's architecture class">
    <meta name="robots" content="all">
    <meta name="author" content="Tiger Shen">
    
    <meta name="keywords" content="">
    <link rel="canonical" href="http://tigerthinks.com/2017/03/26/bradfield-architecture/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Tiger Shen" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?202011300937" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic&amp;subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300&amp;subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    
    

    <!-- MathJax -->
    
    <script type="text/javascript" async
        src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Computer Architecture and the Hardware/Software Interface">
    <meta property="og:description" content="Tiger Shen">
    <meta property="og:url" content="http://tigerthinks.com/2017/03/26/bradfield-architecture/">
    <meta property="og:site_name" content="Tiger Shen">
    
    <meta property="og:image" content="http://tigerthinks.com/images/me.jpg">
    

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Computer Architecture and the Hardware/Software Interface" />
    <meta name="twitter:description" content="Lecture notes from Bradfield's architecture class" />
    <meta name="twitter:url" content="http://tigerthinks.com/2017/03/26/bradfield-architecture/" />
    
    <meta name="twitter:image" content="http://tigerthinks.com/images/me.jpeg" />
    

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="76x76" href="/favicon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">

    
</head>


<body>
  
  

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://tigerthinks.com/" class="site-title">Tiger Shen</a>
      <nav class="site-nav">
        <a href="/blog">
  Personal
</a>

<a href="/tech-blog">
  Tech
</a>

<a href="/anki">
  Anki
</a>

<a href="#">
  |
</a>

<a href="/books/top">
  Books (43)
</a>

<a href="/articles/top">
  Articles (119)
</a>

<a href="/other/top">
  Other (151)
</a>

      </nav>
      <div class="clearfix"></div>
      

      <span id="search-searchbar"></span>

    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        <div class="post-list" id="search-hits">
        </div>

        <div id="content">
          


<div class="post-header mb2">
  <h1>
    
    <div class="clearfix mxn2">
      <div class="col col-2 sm-col-12 px2">
        <img class="inline" src="/images//courses/bradfield.png" alt="Book Cover">

      </div>
      <div class="col col-10 sm-col-12 px2">
        Computer Architecture and the Hardware/Software Interface
      </div>
    </div>
    
  </h1>
  <div class="tags">
  
    <a href="/course" class="badge badge-blue-grey-base">course</a>
  
    <a href="/technical" class="badge badge-deep-orange-base">technical</a>
  
    <a href="/programming" class="badge badge-lime-200">programming</a>
  
    <a href="/architecture" class="badge badge-yellow-200">architecture</a>
  
    <a href="/cs" class="badge badge-deep-orange-100">cs</a>
  
    <a href="/bradfield" class="badge badge-amber-a200">bradfield</a>
  
</div>

  <span class="post-meta">By Bradfield | <a target="_blank" href="https://bradfieldcs.com/courses/architecture/">Course Page</a> | Taken 2017</span><br>
  
  <span class="post-meta small">
  
    36 minute read
  
  </span>
</div>

<article class="post-content">
  <b> <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20">  <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20">  <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20">  <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20">  <img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20"> </b>

  <h2 id="notes">Notes</h2>

<p>Everything is bits!!</p>

<h3 id="1-the-big-picture">1: The Big Picture</h3>

<p>concepts that map to one instruction fall on the “hardware” side of the
hardware/software boundary
arithmetic: signed/unsigned additon, subtraction, multiplication,
division
bitwize operations: and, or, xor etc
read/write (load/store) of register values (words) to/from main memory
cpu makers continue to add new low level instructions to their hardware:
<!--https://en.wikipedia.org/wiki/Bit_Manipulation_Instruction_Sets#BMI2_.28Bit_Manipulation_Instruction_Set_2.29-->
concepts that decompose into many carefully arranged instructions fall
on the “software” side
objects, arrays, methods, subroutines, functions</p>

<p>Jon Carmack has a good heuristic for how much stuff a CPU supports doing
natively hardware: how long would it take you to write an emulator for
it in software? Another good heuristic is to weigh the size of the
manuals :)</p>

<p>binary encoding</p>

<p>the contents of registers, main memory, files (and the disk in general)
is always bit patterns. or put another way: nothing isn’t a bit pattern
the bit patterns only “mean” something in some context
a good example here is that output byte stream of the shell is hooked up
to the terminal, which is always interpreting bytes as teleprinter
instructions
so if some program is not generating binary data that corresponds to
values in the ascii table, and you feed that programs output to a
terminal (which is expecting bytes that do) you will get nonsense
(question marks, blanks, maybe the bell will ring)</p>

<p>the nybble to bit pattern table</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----  : 0       0000 = 00
---+  : 1       0001 = 01
--+-  : 2       0010 = 02
--++  : 3       0011 = 03
-+--  : 4       0100 = 04
-+-+  : 5       0101 = 05
-++-  : 6       0110 = 06
-+++  : 7       0111 = 07
+---  : 8       1000 = 08
+--+  : 9       1001 = 09
+-+-  : a       1010 = 10
+-++  : b       1011 = 11
++--  : c       1100 = 12
++-+  : d       1101 = 13
+++-  : e       1110 = 14
++++  : f       1111 = 15
</code></pre></div></div>

<p>some programs related to last night’s exercises</p>

<p>plain linear bitdump. we’re basically asking for no formatting, just the
raw bitstream</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ruby -e '
  puts(            # print argument bytes to stdout, followed by newline
byte
   ARGF            # file/stdin stream
    .read          # byte contents of stream (as ruby string type)
    .unpack("B*")  # convert bytes of string to binary string
representations
  )
  ' &lt;&lt;&lt; 'hello world' # feed ascii bytes for hello world (plus newline)
as input on stdin
011010000110010101101100011011000110111100100000011101110110111101110010011011000110010000001010
</code></pre></div></div>

<p>obviously that’s very hard for our wetware to interpret which is why we
hexdump</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cat &gt; hexdump.rb &lt;&lt;&lt;'                 # save string in single quotes
to hexdump.rb
  ARGF.each_byte
    .map{|b|b.to_s(16).rjust(2,"0")}    # byte as hex digits
    .each.with_index                    # include indexes
    .each_slice(4)                      # groups of 4
    .each{|s| send :puts,               # print line with:
      s[0][1].to_s.rjust(6) +           #  first byte index, padded
      "  " +                            #  two spaces
      s.map{|b,_|b}.join(" ")           #  bytes, joined with spaces
    }
  ' &amp;&amp; ruby hexdump.rb &lt;&lt;&lt; 'hello world'
     0  68 65 6c 6c               &lt;- bytes at index zero
     4  6f 20 77 6f               &lt;- bytes starting at index 4
     8  72 6c 64 0a               &lt;- bytes starting at index 8
</code></pre></div></div>

<p>and while we’re trying to memorize those nybble to bit pattern
correspondences it’s useful to bitdump</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat &gt; bitdump.rb &lt;&lt;&lt;
  ARGF.each_byte
    .map{|b|
      b.to_s(2).rjust(8,"0")          # eight ones and zeros
       .gsub("1","+").gsub("0","-")   # convert to +/-
    }
    .each.with_index                  # indexes
    .each_slice(4)                    # groups of 4
    .each{|s| send :puts,             # print line with
      s[0][1].to_s.rjust(6) +         #  byte index
      "  " +                          #  two spaces
      s.map{|b,_|b}.join(" ")         #  bytes separated with spaces
    }
 &amp;&amp; ruby bitdump.rb x
     0  -++-+--- -++--+-+ -++-++-- -++-++--
     4  -++-++++ --+----- -+++-+++ -++-++++
     8  -+++--+- -++-++-- -++--+-- ----+-+-
</code></pre></div></div>

<p>hexload is useful for feeding raw bytes that don’t correspond to any
keys on the keyboard straight into the terminal</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cat &gt; hexload.rb &lt;&lt;&lt; 
print ARGF.read         # read contents of input file to string
  .split(/\s/)          # split on whitespace
  .map{|s|s.to_i(16)}   # convert from hex strings to ints
  .pack("C\*")           # convert int array into ascii string

$ ruby hexload.rb &lt;&lt;&lt; '68 65 6c 6c 6f 20 77 6f 72 6c 64 0a'
hello world
</code></pre></div></div>

<p>remember, you can get to the ascii table with man ascii</p>

<p>test your twos-compliment conversion skills online:
http://www.free-test-online.com/binary/two_complement.htm</p>

<p>endianess
remember “LLL” (triple L):
IF the byte with the (L)owest index (i.e. the “(L)eftmost”)
is the (L)east significant byte
THEN the byte order is (L)ittle endian
otherwise it must be big endian</p>

<p>you can’t tell endianness by looking at just the bytes, someone has to
say “these 4 bytes 50 d6 12 00 encode the number 1234512 and then you
can say “oh it’s little endian”</p>

<h3 id="2-overview-of-c-jk">2: Overview of C (jk)</h3>

<ul>
  <li>Bit fields are sets of bits where each position means something
    <ul>
      <li>Why would you condense stuff into bit fields? Because <em>MEMORY ACCESS
IS THE MOST EXPENSIVE THING A CPU CAN DO</em> and you’d reduce how often
this happens</li>
    </ul>
  </li>
  <li>6 bitwise ops
    <ul>
      <li>XOR NOT AND OR SLL (shift left logical) SRL (shift right logical)</li>
    </ul>
  </li>
  <li>Floats are crazy
    <ul>
      <li>Bit field with 1 sign bit, a set of exponent bits, and an unsigned
part</li>
      <li>Arithmetic gets crazy since floats are crazy</li>
      <li>Implemented by multiplying the unsigned by the exponent (which is
two’s complement so can be negative) and then applying the sign bit</li>
      <li>Better to use decimals that don’t repeat by default instead of
floats by default</li>
    </ul>
  </li>
  <li>Unicode! Because ASCII always had a leading 0 and didn’t have enough
space for all the characters (only 255)
    <ul>
      <li>Good thing they did is assign every character (not just English
letters) a code. UNICODE IS A CHARACTER SET UTF-8 IS A SET OF BYTE
ENCODINGS FOR THOSE CHARACTERS</li>
      <li>Bad thing they did is come up with a new 2 byte encoding.. 2 bytes =
64,000 possibilities. Not enough!</li>
      <li>Still use Unicode mapping of characters to numbers, but for byte
encoding we use UTF-8</li>
      <li>UTF-8 pattern: if starts with 110, then follows ONE AND ONLY ONE
byte that MUST start with 10. If starts with 1110, then two bytes
follow that start with 10</li>
    </ul>
  </li>
  <li>To print out a binary file: use <code class="language-plaintext highlighter-rouge">strings a.out</code>
</li>
  <li>
<code class="language-plaintext highlighter-rouge">xxd</code> hexdumps a file
    <ul>
      <li>“magic number” at the beginning of every binary file signifies the
file/encoding type</li>
      <li>Bunch of shit at the top of the file, meant for the kernel. Mach-O
headers, then load commands (instructions for kernel to prep for the
program)</li>
    </ul>
  </li>
</ul>

<h3 id="3-mips">3: MIPS</h3>

<ul>
  <li>Stored programs are <em>separate</em> from the memory. Not in use rn rly,
most stuff is loaded from disk into memory</li>
  <li>When your program runs the OS allocates space for ya</li>
  <li>CPU and OS collaborate on how to organize memory</li>
  <li>Segments of memory
    <ul>
      <li>Stack: contains stuff like function calls, local variables, etc.</li>
      <li>Text (contains the contents of the running program. Most OS’s don’t
let you write here)</li>
      <li>Data (global constant, declare data)</li>
      <li>Kernel (stuff that the program/OS can interact with close to metal)</li>
      <li>I/O: constant</li>
      <li>Heap: Grows as you ask the OS for more memory. How big is you heap?
As big as you’ve asked for as your program is running
        <ul>
          <li>Heap grows down</li>
          <li>
<code class="language-plaintext highlighter-rouge">malloc</code> adds heap memory, <code class="language-plaintext highlighter-rouge">free</code> removes heap memory</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Jumps are short (relative bytes) or long (specific address)</li>
  <li>Special pointers
    <ul>
      <li>Stack pointer is end of stack (stack grows up)</li>
      <li>Instruction pointer is next instruction (usually in text but can go
to heap)</li>
      <li>Global pointer before data segment</li>
      <li>Heap pointer pointer to next spot you can add more heap to</li>
    </ul>
  </li>
  <li>MIPS is big endian</li>
  <li>Instruction set
    <ul>
      <li>add/sub/div</li>
      <li>jump</li>
      <li>load/store</li>
      <li>and/or/shift</li>
    </ul>
  </li>
  <li>You can copy by adding to 0b00000000
    <ul>
      <li>In MIPS r0 is 0</li>
    </ul>
  </li>
  <li>RISC vs CISC: reduced vs complex instruction sets</li>
  <li>MIPS command
    <ul>
      <li>First 5 bits is the op code (only 32 ops!)</li>
      <li>Register command, immediate command, jump command have different stuff
following op code</li>
    </ul>
  </li>
</ul>

<h4 id="berkeley-lecture-videos">Berkeley Lecture Videos</h4>

<ul>
  <li>https://www.youtube.com/watch?v=zUYCZYKaUrk</li>
  <li>
<em>Instructions</em> are primtive ops CPU may execute</li>
  <li>Early on, adding more instructions to instruction set. Helps with
vendor lock-in
    <ul>
      <li>Led to RISC to address bloated CPUs; keep instruction set small and
make it fast</li>
      <li>Leave it up to software to do complicated ops</li>
      <li>MIPS is the company that built a commercial RISC architecture</li>
    </ul>
  </li>
  <li>“Variables” in assembly are registers
    <ul>
      <li>Supah fast (&lt; 1ns access)</li>
    </ul>
  </li>
  <li>32 registers because of goldilocks cool
    <ul>
      <li>Each 32-bit register is a <em>word</em>
</li>
      <li>Each register got a name, use names!</li>
    </ul>
  </li>
  <li>
<em>Immediates</em> are numerical constants :) 1, 20, 30, 1512, etc.
    <ul>
      <li>
<code class="language-plaintext highlighter-rouge">addi</code> to add immediate</li>
      <li>$zero gets its own register</li>
    </ul>
  </li>
  <li>Overflows yikes
    <ul>
      <li>addu, subu, etc. do not detect overflow</li>
      <li>add, addi, sub, subi, etc. do detect overflow</li>
    </ul>
  </li>
  <li>Store extra data in memory
    <ul>
      <li>Memory addresses are in bytes</li>
      <li>Always fetch a whole word from memory (lw is load word)</li>
      <li>Before parens is the offset; in an array, <code class="language-plaintext highlighter-rouge">lw $t0, 12($s3)</code> gets
index 3 (12 is 4 * 3 which offsets you by 3 indices)</li>
    </ul>
  </li>
  <li>lb “sign-extends” a byte by putting the index 7 bit in the rest ofthe
empty space. lbu does not sign-extend</li>
  <li>Registers are 100-500 times faster than memory</li>
  <li>Shift right arithmetic (sra) preserves sign to be shifted (fills in 1s
to the left side)</li>
  <li>Branching for conditionals in MIPS: <code class="language-plaintext highlighter-rouge">beq register1, register2, label</code>
    <ul>
      <li>Unconditional branch is a jump (<code class="language-plaintext highlighter-rouge">j</code>)</li>
    </ul>
  </li>
  <li>Computer words are instructions, vocabulary is instruction set</li>
  <li>Assembly code is assembled into object files, which are “linked” to
machine code executable files</li>
  <li>VMM (virtual memory manager) built into CPU for managing virtual
memory space efficiently</li>
  <li>To branch on equality, use bne and go to Exit label if u dead</li>
  <li>slt reg1, reg2, reg3
    <ul>
      <li>Set Less Than</li>
      <li>reg2 &lt; reg3 ? reg1 = 1 : reg1 = 0</li>
      <li>slti uses immediate</li>
    </ul>
  </li>
  <li>Fundamental steps in calling a function
    <ul>
      <li>These all happen in high level programs but we don’t think about
them as much</li>
      <li>Put paramteres where function can access them</li>
      <li>Transfer control to function</li>
      <li>Acquire resources needed for function</li>
      <li>Perform task of function</li>
      <li>Put result somewhere calling code can find it, restore stack
registers</li>
      <li>Return control to point of origin (jump back)</li>
    </ul>
  </li>
  <li>In MIPS:
    <ul>
      <li>$a* registers are for arguments</li>
      <li>$v* registers are for value registers to return</li>
      <li>$ra is return address to hop back to</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">jal</code> is to jump and link!</li>
  <li>So, caller puts params in registers $a0-$a3 then uses jal X to invoke
X
    <ul>
      <li>jal puts the <em>address of the next instruction</em> into $ra</li>
    </ul>
  </li>
  <li>To save old register values after function call ($s* registers), you
gotta save those somewhere and restore after
    <ul>
      <li>Use a stack!</li>
      <li>Grow from high to low address; so push decrements $sp (stack
pointer), pop increments it</li>
      <li>MIPS only tells you to save $s0 to $s7
        <ul>
          <li>use sw to store stuff in the stack before your function, then lw
to bring them back after</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="4-compiling-linking-assembling-loading">4: Compiling, Linking, Assembling, Loading</h3>

<h4 id="pre-work">Pre-work</h4>

<ul>
  <li>https://www.youtube.com/watch?v=Z4r9AWu8D18</li>
  <li>Translate vs. interpret
    <ul>
      <li>High-level languages are interpreted, which means they are executed
by another program</li>
      <li>Low level languages are translated into an intermediate step</li>
      <li>Interpreting 10-100x slower</li>
    </ul>
  </li>
  <li>C is compiled :). Let’s compile foo.c
    <ul>
      <li>foo.c</li>
      <li>Through compiler</li>
      <li>Assembly program: foo.s</li>
      <li>Through assembler</li>
      <li>Object (mach lang module): foo.o</li>
      <li>Linker</li>
      <li>Executable (mach lang program): a.out</li>
      <li>Loader</li>
      <li>Come to memory papa</li>
    </ul>
  </li>
  <li>Compiler
    <ul>
      <li>Input is C code, output is assembly code</li>
      <li>Might produce pseudoinstructions like <code class="language-plaintext highlighter-rouge">move</code> (add 0 and copy)</li>
    </ul>
  </li>
  <li>Assembler
    <ul>
      <li>Input is assembly language</li>
      <li>Output is object code</li>
      <li>Reads and uses directives (.text, .data, .asciiz, etc.)</li>
      <li>Expand pseudoinstructions
        <ul>
          <li>E.g. no subu, so do addiu with negative value</li>
          <li>Multiplication: m x n = m + n bits product
            <ul>
              <li>Result goes into hi and lo; hi is the upper half, lo is the
lower half</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Produce machine language
        <ul>
          <li>Wat do</li>
          <li>Simple case is just arithmetic, shifts, logic, etc. Easy</li>
          <li>Branches tough :( Relative to where your pc is
            <ul>
              <li>Where is the label you want to jump to? Solved by taking two
passes over the program, once to remember position and other
using those label positions to generate code</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Object file
        <ul>
          <li>object file header: size and position of other stuff in file</li>
          <li>text segment: machine code</li>
          <li>data segment: binary static data</li>
          <li>relocation info: identifies lines of code to be fixed up (i.e.
include directive)</li>
          <li>symbol table: list of labels and static data references</li>
          <li>debugging info</li>
          <li>Standard format is ELF</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Linker
    <ul>
      <li>Combines a ton of .o files to make one a.out file</li>
      <li>Input is .o files</li>
      <li>Output is executable code</li>
      <li>Combines several files into a single executable
        <ul>
          <li>Enables separate compilation of stuff, so you can change and
recompile one file without doing the whole project</li>
        </ul>
      </li>
      <li>Takes the text/data/info from multiple o files and then sticks them
together interleaved with each other
        <ul>
          <li>Resolves references in files. Go thru Relocation Table, look at
each entry, and repace with absolute address
            <ul>
              <li>PC-relative address (beq, bne) never relocated</li>
              <li>Absolute Function address (j, jal), External Function reference
(jal), Static Data reference (lui, ori) always relocated</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>We assume the first word of first text segment is at 0x04000000
(stuff below is reserved)</li>
      <li>We know how long each text and data segment is, and how they are
ordered</li>
      <li>So we calculate the absolute address of each label and each piece of
data being referenced after concatting</li>
      <li>Resolve references:
        <ul>
          <li>Search for reference in symbol tables</li>
          <li>if not there, search lib files</li>
          <li>Finally, fill in correct machine code</li>
        </ul>
      </li>
      <li>Final output is machine executable file with header</li>
    </ul>
  </li>
  <li>Loader
    <ul>
      <li>Takes executable code and runs it</li>
      <li>Executable code is on disk, loader needs to load into memory and run</li>
      <li>Read header to see how big file is</li>
      <li>Assign amount of memory for each piece - text, data, stack</li>
      <li>Copies instruction and data into address space</li>
      <li>Copies arguments onto stack</li>
      <li>Initializes machine registers to be usado</li>
      <li>Jumps to start-up routine that copies program’s arguments from stack
to registers and sets the program counter</li>
    </ul>
  </li>
  <li>This naive approach is statically-linked. We bring in the entire
    <stdio> library even if not all is used
</stdio>
    <ul>
      <li>Alternative is dynamically linking, which is common oon UNIX and
stuff</li>
    </ul>
  </li>
</ul>

<h4 id="building-mach-o-executable">Building Mach O executable</h4>

<ul>
  <li>https://www.mikeash.com/pyblog/friday-qa-2012-11-30-lets-build-a-mach-o-executable.html</li>
  <li>PAGEZERO load command blocks off lower 4GB of memory space, so that
dereferencing NULL pointers causes a segmentation fault
    <ul>
      <li>What means? This is the __PAGEZERO segment, which predefines the
entire lower 4GB of the 64-bit virtual memory space as inaccessible.
Because of this segment, which is marked unreadable, unwriteable, and
nonexecutable, dereferencing NULL pointers causes an immediate
segmentation fault.</li>
    </ul>
  </li>
  <li>Load Commands - it’s kind of a table of contents, that describes
position of segments, symbol table, dynamic symbol table etc. Each
load command includes a meta-information, such as type of command, its
name, position in a binary and so on.</li>
</ul>

<h4 id="lecture">Lecture</h4>

<ul>
  <li>Compiling a C file goes a long ways…
    <ul>
      <li>
<code class="language-plaintext highlighter-rouge">cc</code> is just a frontend for clang and ld (linker)</li>
    </ul>
  </li>
  <li>See compiling_step_by_step.sh for deets</li>
</ul>

<h3 id="5-the-processor-clock-and-datapath">5: The Processor, Clock, and Datapath</h3>

<h4 id="prework-video">Prework video</h4>

<ul>
  <li>https://www.youtube.com/watch?v=OOBwKAXZjlk</li>
  <li>Don’t need much to run software in hardware!</li>
  <li>Take physical device and run programs on it? How?</li>
  <li>Let’s do adder/subtractor</li>
  <li>Start with truth table, minimize and implement as we’ve seen before
    <ul>
      <li>Solve the subproblem! Add 1 bit before thinking about 32 bit</li>
    </ul>
  </li>
  <li>Ok so let’s think about instructions for adding 1 bit 3 times
    <ul>
      <li>Sum of three bits is XOR(a, b, c) (with three inputs, odd number of
1s is a 1, even number of 1s is a 0)</li>
      <li>The carry is MAJ(a, b, c) which is a&amp;b + a&amp;c + b&amp;c (the sum of
these)</li>
    </ul>
  </li>
  <li>N 1-bit adders =&gt; 1 N-bit adder</li>
  <li>But you need to worry about overflow
    <ul>
      <li>If last carry bit is 1, you have overflow</li>
    </ul>
  </li>
  <li>To do subtractor, take first number and add to negative of second
number
    <ul>
      <li>Negative done by two’s complement</li>
      <li>Add another bit input to adder to designate whether second input
should be flipped</li>
      <li>AN XOR SERVES AS A CONDITIONAL INVERTER CAUSE YOU CAN INVERT EACH
BIT</li>
      <li>THEN TAKE THE SUM AND SUBTRACT ONE AND YOU’RE GUCCI</li>
    </ul>
  </li>
  <li>Components</li>
  <li>Processor has a control and a datapath
    <ul>
      <li>Control tells datapath what to do (what registers to read, which
operation to perform)</li>
      <li>Datapath includes PC, Registers, ALU</li>
    </ul>
  </li>
  <li>Processor connects to memory</li>
  <li>Memory connects to I/O</li>
  <li>So there’s two boundaries: processor-memory interface and memory-I/O
interface</li>
  <li>CPU
    <ul>
      <li>Processor is the CPU, active part of the computer that does the work
        <ul>
          <li>Datapath contains hardware to perform operations</li>
          <li>Control: tells the datapath what needs to be done (brains)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>How to execute instruction?
    <ul>
      <li>Fetch: fetch the 32-bit instruction word from memory, returns them in the
control unit, and then increment program counter</li>
      <li>Decode: look at 32-bit instruction and figure out what it means.
        <ul>
          <li>First thing is obviously the opcode.</li>
          <li>Then, read data from necessary registers (2 for add, 0 for jump)</li>
        </ul>
      </li>
      <li>ALU: do work like arithmetic, shift, logic. Also needs to be done
for lw and sw to add the offset to the address</li>
      <li>Memory access: only loads and stores do stuff here. Should be fast
        <ul>
          <li>To store, need to read a) base address, b) data value to store, c)
immediate offset</li>
        </ul>
      </li>
      <li>Register write: write result back to register
        <ul>
          <li>For store instruction that writes to memory, work has been done in
last stage</li>
        </ul>
      </li>
      <li>Many instructions don’t use all stages!</li>
    </ul>
  </li>
  <li>Lecturer walks through these steps, makes sense</li>
  <li>Immediate values come out of instruction memory</li>
  <li>Not all instructions use all stages, but for MIPS at least it’s the
union of all the operations needed by all instructions
    <ul>
      <li>Load instruction uses all 5 :)</li>
    </ul>
  </li>
  <li>Datapath and control bois
    <ul>
      <li>Datapath-&gt;control feedback is from instruction memory</li>
      <li>Controller makes sure right things happen at right time, can hook
into all parts of datapath</li>
    </ul>
  </li>
  <li>Processor design
    <ul>
      <li>Analyze instruction set to determine datapath requirements. Must
support everything!</li>
      <li>Select set of datapath components and establish clocking. Things
happen on the rising edge</li>
      <li>Assemble datapath components</li>
      <li>Analyze implementation of instructions and set up control points</li>
      <li>Assemble control logic (formulate equations, design circuits)</li>
    </ul>
  </li>
  <li>3 types of MIPS instructions
    <ul>
      <li>R type has op, register s, register t, register destination, shamt (shift
amount), funct
(add, subtract, etc.)</li>
      <li>I type has op, register source, register taret, immediate</li>
      <li>J type has op code then target address</li>
    </ul>
  </li>
  <li>Register Transfer Langauge is a way of writing down what happens
during execution of each instruction
    <ul>
      <li>Pseudocode ish. For ADDU instruction, RTL is like this: R[rd] &lt;–
R[rs] + R[rt]; PC &lt;– PC + 4</li>
      <li>All instructions start by fetching instruction itself</li>
    </ul>
  </li>
  <li>Requirements of instruction set for our MIPS light: stuff like MEM,
Registers, PC, sign extender, ALU, PC incrementer, etc.</li>
  <li>So now for our components. Need combinatorial elements (don’t respond
to clock) and storage/sequential elements (respond to clock)</li>
  <li>Describes his class’s architecture cool</li>
  <li>Clock stuff
    <ul>
      <li>“Critical path” (longest path through logic) determines the length
of the clock period</li>
      <li>Art of hardware design is moving clock edges closer together,
shortening critical path</li>
    </ul>
  </li>
  <li>State machine that reads the instruction, updates state, then awaits
next instruction. Cool</li>
</ul>

<h4 id="lecture-1">Lecture</h4>

<ul>
  <li>Hertz is number of switches per second</li>
  <li>Clock cycle
    <ul>
      <li>Starts with rising edge with high current, then it has a down edge
with lower current, then back to rising edge, ez</li>
    </ul>
  </li>
  <li>To get faster, reduce critical path speed OR add flip flops/registers
in the middle to save work</li>
  <li>All digital systems with time:
    <ul>
      <li>Current state sent from stateful part to combinatorial logic along
with static inputs</li>
      <li>Combinatorial logic does work and emits outputs, and next state is
sent to the clocked chip</li>
      <li>“Next state” from previous step is the state passed in during the
next clock cycle</li>
    </ul>
  </li>
  <li>C Pro Tip
    <ul>
      <li>Read <code class="language-plaintext highlighter-rouge">char *argv[]</code>. WHEN you invoke *argv, you get the other part
of the expression, which is <code class="language-plaintext highlighter-rouge">char[]</code>
</li>
    </ul>
  </li>
</ul>

<h3 id="6-using-logic-gates-to-build-logic-gates">6: Using Logic Gates to Build Logic Gates</h3>

<h4 id="prework-video-1">Prework video</h4>

<ul>
  <li>https://www.youtube.com/watch?v=SstCrz0xUzw</li>
  <li>Why study hardware even if you don’t work on hardware? Want to
understand capabilities and limitations so you can utilize hardware
effectively</li>
  <li>Basics of a computer system is a <em>synchronous digital system</em>
    <ul>
      <li>Synchronous: all operations coordinated by central clock</li>
      <li>Digital: all values are discrete value (analog = voltage, etc.)
        <ul>
          <li>Binary (0, 1). Electrical signals are 1 and 0 (high and low
voltage)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Implement a circuit/switch
    <ul>
      <li>If you close a switch and complete a loop, then current flows and
lightbulb can be on</li>
      <li>Boolean logic based on Boole lol</li>
    </ul>
  </li>
  <li>Transistors used to represent high/low voltage (they’re the switches
in computers)
    <ul>
      <li>Remove noise by setting a midpoint voltage; above is 1, below is 0</li>
      <li>CMOS is ours: Complementary Metal Oxide on Semiconductor</li>
    </ul>
  </li>
  <li>n-type transistor is open when no voltage, closed when voltage. p-type
is opposite
    <ul>
      <li>Basically you can use complementary pairs to get strong signals</li>
    </ul>
  </li>
  <li>https://gyazo.com/2cc4aaf3f68f6495f7646d6200068cd7</li>
  <li>NAND!</li>
  <li>Some combinatorial logic symbols are standard zzz</li>
  <li>Truth tables describe the inputs and outputs of a circuit</li>
  <li>Simplifying boolean logic is an art form rofl</li>
  <li>Boolean Algebra
    <ul>
      <li>
        <ul>
          <li>for OR (logical sum)</li>
        </ul>
      </li>
      <li>dot for AND (logical product)</li>
      <li>Hat for NOT (complement/negation)</li>
    </ul>
  </li>
  <li>Bunch of laws of boolean algebra</li>
  <li>Signals and waveform stuff
    <ul>
      <li>Can look across separate wires to aggregate a signal</li>
    </ul>
  </li>
  <li>
    <p>Propagation delay is difference between changing input and changing
output</p>
  </li>
  <li>Synchronous digital systems help abstract time/delays
    <ul>
      <li>Come with two types of circuits</li>
      <li>Combinatorial logic: output is a pure function of the inputs,
doesn’t have history of execution</li>
      <li>Sequential logic: circuits that remember or store information
across time. Clocks synchronize systems!</li>
    </ul>
  </li>
  <li>Slides are helpful</li>
  <li>https://gyazo.com/738b402a3fef46496bdfd7c8acc3fed6</li>
</ul>

<h4 id="lecture-2">Lecture</h4>

<ul>
  <li>
    <p>Flip flop circuits are like camera shutters: open, snapshot, emit,
close, etc</p>
  </li>
  <li>CLOCKS ARE IMPORTANT. THEY MAKE EVERYTHING GO</li>
  <li>SIMD: Single Instruction Multiple Data
    <ul>
      <li>Can load 4 32-bit ints into a 128-bit register, then can do four
adds in parallel</li>
    </ul>
  </li>
  <li>
    <p>Think about flow of electricity to model voltage in circuits</p>
  </li>
  <li>How is NAND implemented in electric circuits?</li>
</ul>

<h3 id="7-pipelining">7: Pipelining</h3>

<h4 id="video">Video</h4>

<ul>
  <li>https://www.youtube.com/watch?v=oIawE3IseRA</li>
  <li>Single Cycle processor review
    <ul>
      <li>See “processor design” segment</li>
      <li>End up with cool datapath</li>
      <li>Performance: for every instruction, need to wait until worst case
time for worst instruction. Clock rate (cycles/second = Hz) =
1/period (seconds/cycle)</li>
    </ul>
  </li>
  <li>Pipeline increases clock rate over worst case performance
    <ul>
      <li>Increased clock rate means faster programs hopefully</li>
    </ul>
  </li>
  <li>Can overlap the stages of stuff to make it more efficient
    <ul>
      <li>Analogy is laundry (washer, dryer, folder, stasher). Sequentially
takes two hours, but if you do batches with pipelining you get more
efficient. After first load is washed, you load it into the dryer
and immediately add second load to washer</li>
    </ul>
  </li>
  <li>Pipelining does <em>not</em> help latency of single task, it helps
<em>throughput</em> of entire workload</li>
  <li>
<em>Multiple</em> tasks operating simultaneously using different resources</li>
  <li>Potential speedup = the number of pipe-able stages</li>
  <li>Time to “fill” pipeline and time to “drain” it reduces the speedup :)</li>
  <li>Pipeline limited by slowest pipeline stage (still better than being
limited by the slowest entire instruction, which you are without
pipelining)
    <ul>
      <li>Try to balance lengths of pipe stages</li>
    </ul>
  </li>
  <li>Apply pipeline to MIPS assembly
    <ul>
      <li>Just add registers between stages (fetch, decode, execute, memory,
write back)</li>
      <li>These registers hold information produced by previous cycle</li>
    </ul>
  </li>
  <li>Need to keep a copy of instruction bits and move them down the
pipeline so each piece knows exactly what to do with that instruction</li>
  <li>
    <p>Several ways to represent pipeline (graphical, etc)</p>
  </li>
  <li>Pipelining performance</li>
  <li>Best case is Time of single cycle / number of stages (equality is only
achieved if stages are balanced)</li>
  <li>Speedup reduced if not equally balanced</li>
  <li>Remember, pipelining increases throughput not latency</li>
  <li>
    <p>Pipelining increases instruction latency (must match longest
instruction latency), does not increase number of
components</p>
  </li>
  <li>Pipelining hazards precent starting the next instruction in next clock
cycle</li>
  <li>Structural hazard: required resource is busy (e.g. needed in multiple
stages)
    <ul>
      <li>e.g. Multiple registers need to be write/read simultaneously</li>
      <li>Easy-ish to solve</li>
      <li>Keep separate caches for instruction fetch and memory RW</li>
      <li>Split RegisterFile access in two: write during 1st half and read
during 2nd half of each clock cycle</li>
      <li>So, read and write to registers during same clock cycle is okay</li>
      <li>Can always be removed by adding hardware resources</li>
    </ul>
  </li>
  <li>Data hazard: data dependency between instructions, need to wait for
previous instruction to finish up
    <ul>
      <li>Data flow <em>backwards</em> is a hazard (e.g. an add happens in one
instruction, a bunch of subsequent instructions need to use the
value produced by that add</li>
      <li>Cool. Register forwarding: forward result as soon as it’s available,
even if it’s not stored in RegFile yet. Add a sneak path for
forwarding value from output of ALU to input of ALU without
writing/reading registers or memory</li>
      <li>What’s the datapath for forwarding?</li>
      <li>Add a forwarding unit that checks source registers, compares them to
registers written in earlier instructions, and if they match, then
you do the forwarding. E.g. if $t0 written in one add (destination),
then $t0 used in subsequent subtract (source), then mux in the new value</li>
      <li>Loads are tough, cause you need the memory value. Can’t forward,
need to wait until load value is actually available. Must stall
instruction dependent on load, and then forward</li>
      <li>Called <em>hardware interlock</em> when hardware stalls pipeline
        <ul>
          <li>Replace stalled instruction with “bubble”, which is a no-op</li>
        </ul>
      </li>
      <li>Slot after a load is a load delay slot, can’t use loaded value for
one slot.
        <ul>
          <li>Don’t try and use a value once cycle after load :) <code class="language-plaintext highlighter-rouge">nop</code> instead</li>
        </ul>
      </li>
      <li>MIPS doesn’t have interlocked pipelining stages :)
        <ul>
          <li>But adds back interlock because it’s smelly to nop everywhere</li>
        </ul>
      </li>
      <li>Compiler can help with hardware interlock by inserting unrelated
instruction into that space so you can take advantage of nop time
        <ul>
          <li>Can save stuff</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Control hazard: Flow of execution depends on previous instruction
(branch or jump)
    <ul>
      <li>Branch determines flow control</li>
      <li>Simple solution option 1: just stall on every branch instruction
until branch resolved
        <ul>
          <li>Adds 2 bubbles/cycles for each branch :( (20% of instructions).
Compare happens at ALU stage, so must wait till then</li>
        </ul>
      </li>
      <li>Optimize: insert special branc comparator after stage 2. Can only do
equality check. Chop penalty down to only one bubble
        <ul>
          <li>RISC is about good pipelining not about few instructions</li>
        </ul>
      </li>
      <li>Optimize: Predict outcome, fix up if guess wrong
        <ul>
          <li>If you’re wrong, must <em>flush</em> pipeline</li>
          <li>Can predict that all branches are NOT taken; just keep going and
fall through :) Only need to flush if branch ends up being taken</li>
        </ul>
      </li>
      <li>Lots of effort spent on this!</li>
      <li>Optimize: can rearrange instructions (compiler) to fill branch delay
with an unrelated, still useful instruction</li>
    </ul>
  </li>
</ul>

<h3 id="8-memory-hierarchy">8: Memory Hierarchy</h3>

<h4 id="mike-acton-data-oriented-design-and-c">Mike Acton Data-Oriented Design and C++</h4>

<ul>
  <li>https://www.youtube.com/watch?v=rX0ItVEVjHc</li>
  <li>On the engine team - supports the runtime systems that games are built
on top of</li>
  <li>Don’t use templates in CPP</li>
  <li>Lots of language features are sad and not used for important stuff</li>
  <li>
<em>Data oriented design</em>: the purpose of all programs is to transform
data from one form into another
    <ul>
      <li>Corollaries: if you don’t understand the data you don’t understand
the problem. You understand a problem better by understanding the
data. If you have different data you have a different problem. If you
don’t understand the cost, you don’t understand the problem. If you
don’t understand the hardware, you can’t reason about the cost of
understanding the problem. Everything is a data problem. :)</li>
      <li>Solving problems you don’t have will add to the number of problems
you do</li>
    </ul>
  </li>
  <li>Where this is one, there are many. Try looking at the most common
problems and stuff first.</li>
  <li>Software does not run in a vacuum!</li>
  <li>Reason must prevail!</li>
  <li>Data-oriented: a reminder of first principles :). Not new ideas at all</li>
  <li>Lies of CPP
    <ul>
      <li>Software is a platform
        <ul>
          <li>Hardware is the platform fam. Reality isn’t some annoying thing
making your solution ugly, reality is the real problem</li>
        </ul>
      </li>
      <li>Code should be designed around your mental model of the world
        <ul>
          <li>Don’t hide data in your mental model! Confuses maintenance with
understanding properties of data (which is critical for solving
problems). Don’t try and idealize the problem</li>
        </ul>
      </li>
      <li>Code more important than data
        <ul>
          <li>No. Code exists to transform data. Programmer responsible for DATA
not code</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Lies lead to poor performance, concurrency, optimizability, stability,
testability. Oops</li>
  <li>
    <p>Solve to transform data you have to where you want it given the
constraints of the platform. Nothing else dude</p>
  </li>
  <li>Solve for most common case first, not most generic
    <ul>
      <li>“Make the compiler do it”. No. Compiler reasons about instructions,
which is only 1-10% of the problem space</li>
    </ul>
  </li>
  <li>Let’s look at memory hierarchy stuff. Much much more expensive to go
to main memory than to have compiler optimize away expensive CPU
instructions. This is an order of magnitude</li>
  <li>Don’t miss the cache!</li>
  <li>
    <p>If cache line is 32 bytes let’s see</p>
  </li>
  <li>Don’t re-read data or re-call functions that you already have answers for</li>
  <li>For bools, using <code class="language-plaintext highlighter-rouge">bool</code> is huge cost because lots of wasted space
    <ul>
      <li>Only fills 1 bit of 512 in the cache line. Can try and squeeze other
stuff in as well</li>
    </ul>
  </li>
  <li>
    <p>Don’t reason about stuff super locally if you can do it at a higher
level</p>
  </li>
  <li>Concentrate on common case first</li>
</ul>

<h4 id="ph-5th-edition-memory-hierarchy-51---54">P&amp;H 5th Edition Memory Hierarchy (5.1 - 5.4)</h4>

<ul>
  <li>Create illusion of unlimited amounts of fast memory</li>
  <li>If you’re writing a paper at a desk, you wanna keep the most important
documents and references close by so you don’t have to keep getting up
to access stuff
    <ul>
      <li>Same principle here: create illusion of large memory by swapping
stuff out of a small memory behind the scenes</li>
    </ul>
  </li>
  <li>Principle of locality. Programs access a small portion of address
space at any given time
    <ul>
      <li>Temporal (in time): if something referenced, likely to be referenced
again. Keep it around</li>
      <li>Spatial (in space): if something referenced, stuff around it is
likely to be referenced soon</li>
    </ul>
  </li>
  <li>Cache faster than SRAM faster than DRAM faster than disk
    <ul>
      <li>S = Static</li>
      <li>D = Dynamic</li>
    </ul>
  </li>
  <li>Take advantage of locality with a hierarchy
    <ul>
      <li>Faster but smaller the closer you get to processor</li>
    </ul>
  </li>
  <li>Data only copied between two adjacent levels at a time</li>
  <li>“Upper” = closer to processor, “Lower” = further away</li>
  <li>Block/line: minimum unit of information that can be present or not
present. Library analogy is a book</li>
  <li>If present, it’s a <em>hit</em>. If not, it’s a <em>miss</em>. Hit rate is the
fraction of accesses that are in the upper level; Miss rate is 1 - hit
rate</li>
  <li>
    <p>Hit time is amount of time taken for a hit. Miss penalty is time to
replace block in upper level with the missed data from lower level,
then deliver data to processor</p>
  </li>
  <li>SRAM are memory arrays that keep values indefinitely as long as power
is applies. DRAM stores stuff as a charge on a capacitator, must be
refreshed (read and write back) periodically. Organized in rows for
read and write (?)</li>
  <li>Hardware stuff over my head</li>
  <li>Flash memory is older stuff, it wears down over time so you have to
use a technique called wear leveling to distribute load over the whole
thing</li>
  <li>Magnetic hard disk supah slow. They are a series of magnetic disks
connected together and moving in conjunction
    <ul>
      <li>Three steps. First, seek, which moves the head to the proper disk
track. Average seek times 3-13ms. Then, when head is on correct
track, must wait for the right sector to rotate to the head.. Average is
like 5ms. Finally, transfer time is how long it takes to move a block to
the head. Transfer rates in 2012 ~ 100-200MB/sec.</li>
    </ul>
  </li>
  <li>
    <p>Primary diff between magnetic disk and semiconductor memory is that
disks are way slower because they’re mechanical.</p>
  </li>
  <li>Caches are memory levels between processor and main memory</li>
  <li>How to find where in cache something is stored? Similar to naive
hashing algorithm: block address module number of blocks in cache.
This is direct mapping</li>
  <li>Add tags to the cache data decribing where it originally lived</li>
  <li>How do you know if cache has valid data? (e.g. stuff can be stale
after a processor startup). Add a <code class="language-plaintext highlighter-rouge">valid</code> bit that is on if entry has
a valid address</li>
  <li>Caching part of <em>prediction</em>. Rely on principle of locality to try and
find desired data, and retrieve the correct data if can’t find it in
the caches. Today computers are 9% cache hit</li>
  <li>Using this modulo approach we’ll have block conflicts. Uhh how to
resolve idk yet</li>
  <li>“The tag from the cache is compared against the upper portion of the
address to determine whether the entry in the cache corresponds to the
requested address. Because the cache has 210 (or 1024) words and a block
size of one word, 10 bits are used to index the cache, leaving 32 −10 −
2 = 20 bits to be compared against the tag. If the tag and upper 20 bits
of the address are equal and the valid bit is on, then the request hits
in the cache, and the word is supplied to the processor. Otherwise, a
miss occurs.”</li>
  <li>Nice diagrams buddy</li>
  <li>Larger block = lower miss rate b/c of spatial locality
    <ul>
      <li>There’s a sweet spot b/t block size + number of blocks that fit in
the cache</li>
      <li>Larger block also means worse miss penalty :( takes longer to load
missed data into the cache</li>
    </ul>
  </li>
  <li>How does processor control handle miss?</li>
  <li>
    <p>Most of the time, just introduce a stall until missed data has been
loaded (if cache hits, just proceed as normal)</p>
  </li>
  <li>Writes different from reads</li>
  <li>After write to memory, cache and memory are inconsistent. Solve this
with write-through: write to both spots each time. But this is sucky
and slow. Need to spend a lot of cycles writing to main memory
synchronously</li>
  <li>Better? <em>Write buffer</em> stores data while waiting to be written to
memory. Processor just needs to write to the buffer (fast), and it
goes from there to main memory. If write buffer is full, processor still
has to wait for free space</li>
  <li>
    <p><em>Write-back</em> also possible. When write occurs, new value only goes to
cache, then written to lower levels when it is replaced. Hard to
implement but better performance</p>
  </li>
  <li>
    <p>How do miss rate and execution time relate to each other? Let’s see</p>
  </li>
  <li>How to measure and improve cache performance</li>
  <li>Two techniques: reduce miss rate by reducing probability that two
different memory blocks dispute over the same cache spot, reduce miss
penalty by adding more levels to hierarchy</li>
  <li>CPU time = (CPU execution clock cycles + Memory-stall clock cycles) x
Clock cycle time</li>
  <li>Memory-stall clock cycles can be read or write. Intuitive equations
for those. Reads: reads/program x read miss rate x read miss penalty</li>
  <li>
    <p>Sometimes can assume hit time is just factored into a clock cycle, but
if you want to get granular about that too you can use a metric called
Average Memory Access Time (AMAT): time for a hit + miss rate x miss
penalty</p>
  </li>
  <li>Can you have same block in multiple locations? i.e. not direct mapped.</li>
  <li>Opposite of direct mapped is <em>fully associative</em>; any block can be
anywhere. Search whole block for what you want</li>
  <li>Middle range is <em>Set associative</em>. Set number of spots where each
block can be placed in n locations (descript would be “an n-way
set-associative cache”)
    <ul>
      <li>Calculate where block can go by (block#) % (#<em>sets</em> in cache)</li>
      <li>Must search all tags in set when looking for a block</li>
    </ul>
  </li>
  <li>Tradeoff of increasing associativity: lower miss rate but increase hit
time</li>
  <li>Definitely diminishing returns; in 64KiB with 16-word block,
associativity from 1 to 2 is like a 1.7% improvement in miss rate, 2
to 3 is like 0.3%</li>
  <li>
    <p>Searching n-associative cache for your data :thinking_face:
Sequential search too slow! Instead, search them all in parallel. Need
one comparator for each level of associativity you add. So cost is extra
hardware + cost of doing compares</p>
  </li>
  <li>Which block do you replace? Need to choose among blocks in set.</li>
  <li>
    <p>Commonly just do LRU</p>
  </li>
  <li>Multilevel cache to improve performance even more</li>
  <li>Primary cache usually smaller than secondary…can use smaller block
size as well. Also lower associativity</li>
  <li>
    <p>Interesting comparison between radix sort and quicksort. Radix sort
algorithmically quicker, but since quicksort has fewer misses per item
it can still perform better</p>
  </li>
  <li>How to use in software?</li>
  <li>
    <p>e.g. if you’re working on an array or list or matrix that won’t all
fit into memory at the same time, instead of going row by row or
column by column you want to go by block size so you can fit the entire
block you’re operating on into memory at the same time. Compute block
size with pointer arithmetic and only fetch what’s necessary. Muy bien</p>
  </li>
  <li>Summary?</li>
  <li>Cache performance, using associativity to reduce miss rates,
multilevel hierarchies to reduce miss penalties, software
optimizations to improve cache usage</li>
</ul>

<h4 id="lecture-3">Lecture</h4>

<ul>
  <li>Registers: 1ns access, ~ 1kb, CMOS tech, managed by compiler/programmer</li>
  <li>L1: 3ns, 32KB, SRAM, CPU</li>
  <li>L2: 6ns, 256KB, SRAM, CPU</li>
  <li>L3: 12ns, 8MB, SRAM, CPU</li>
  <li>
    <p>RAM: 60ns, 16B, DRAM, OS</p>
  </li>
  <li>Half of each cache is for instructions, half for data
    <ul>
      <li>Why? Different data access patterns. e.g. if iterating through
array, iteration code is static (keep the instruction cache), but
get all the new data (evict data cache)</li>
    </ul>
  </li>
  <li>
    <p>Struct packing: wise about ordering of elements in C Structs because
of how compiler will organize your memory</p>
  </li>
  <li>Misses</li>
  <li>Compulsory: nothing in those spots, just go fetch it</li>
  <li>Conflict: gotta try searching in the set</li>
  <li>Evict: least recently used thing tossed out to make room</li>
</ul>

<h3 id="parallelism-flynn-taxonomy-amdahls-law">Parallelism, Flynn taxonomy, Amdahl’s law</h3>

<h4 id="berkeley-lectures-cs-61c-3-31-2015-and-4-2-2015">Berkeley lectures (CS 61C 3-31-2015 and 4-2-2015)</h4>

<ul>
  <li>New school architecture is crazy. Parallelism :thinking_face:</li>
  <li>Use a whole bunch of processors to make things faster</li>
  <li>Two ways:
    <ul>
      <li>Multiprogramming: different independent programs in parallel</li>
      <li>Parallel computing; run multiple at the same time on same machine.
Way harder</li>
    </ul>
  </li>
  <li>SIMD: single-instruction/multiple-data
    <ul>
      <li>Multiple data streams against a single instruction stream. GPU
stuff. Take a “pool” of data and apply same operation to all of them
at the same time</li>
    </ul>
  </li>
  <li>MIMD: multiple processor cores executing different instructions on
different data</li>
  <li>MISD: not very common at all. Odd design</li>
  <li>SIMD/MIMD: Flynn taxonomy.</li>
  <li>Software: SPMD programming. Single Program Multiple Data. Run same
program on different sets of data in different places</li>
  <li>Big idea: Amdahl’s (heartbreaking) law. Speedup due to enhancement E
    <ul>
      <li>Only speedup to parallel steps, suequential steps still neeed to go
sequentially</li>
      <li>Speedup w/E = exec time w/o E DIVIDED BY exec time w/E</li>
      <li>Basically, parallelization speedup is less than you’d intuit</li>
    </ul>
  </li>
  <li>Strong/weak scaling: getting good speedup on parallel processor while
keeping problem size fixed is hardeer than getting good speedup by
increasing the size of the problem</li>
  <li>Strong: speedup can be achieved on parallel processor without
increasing size of problem
    <ul>
      <li>Example: graphics (parts of the screen aren’t dependent on others)</li>
    </ul>
  </li>
  <li>Weak: speedup can be achieved on a parallel processor by increasing
size of problem proportionally to increase in number of processors</li>
  <li>
    <p>Load balancing: each processor should do just about the same amount of
work! Always have to wait for slowest processor</p>
  </li>
  <li>SIMD architecture</li>
  <li>
<em>Data parallelism</em> is executing same operation on multiple data
streams</li>
  <li>Example: multiply a coefficient array by data array (all elements)</li>
  <li>Sources of improvement:
    <ul>
      <li>Only one fetch/decode</li>
      <li>All operations known to be independent</li>
      <li>Pipelining/concurrency in memory access</li>
    </ul>
  </li>
  <li>Intel calls it Advanced Digital Media Boost -_-
    <ul>
      <li>MMX: Multimedia Extensions. Used 64 bit registers that would be
considered broken up. Then parallel ops could be done (1992)</li>
      <li>SSE: Streaming SIMD Extensions: Added 128 bit registers (1999)</li>
      <li>Now AVX: 256 bit registers (2011). Space for expansion to 1024 bit
registers!</li>
    </ul>
  </li>
  <li>Array processing in SIMD</li>
  <li>Without parallelism, need to load each element into float register,
calculate sqrt, write result back</li>
  <li>With parallelism, Load 4 members into the SSE register, calculate 4 in
one operation, stoe them all from register to memory</li>
  <li>This kinda stuff is expressed in programs as for loops
    <ul>
      <li>In MIPS, this would just be a sequential set of instructions as
described above “without parallelism”</li>
      <li>Can unroll a scalar loop to do 4 elements at a time. Only 1 loop
overhead every 4 iterations. Uses different registers for each
iteration to eliminate data hazards in the pipeline
        <ul>
          <li>Now schedule things by doing all loads, then all adds, then all
stores. Cool</li>
          <li>If not in multiple of 4, have a separate loop that handles odds</li>
        </ul>
      </li>
      <li>SIMD this thing by just converting unrolled instructions into one
SIMD instruction
        <ul>
          <li>MOVAPS: move aligned, packed, single. Cool</li>
          <li>ADDPS: add packed single precision</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

</article>













        </div>
      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      © 2018
    </small>
  </div>
</footer>
<!-- AnchorJS -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.0.0/anchor.min.js"></script>
<script>
    anchors.options.visible = 'always';
    anchors.add('article h2, article h3:not(.no-anchor), article h4:not(.no-anchor), article h5:not(.no-anchor), article h6');
</script>


  <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.20.1/moment.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.css">
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch-theme-algolia.min.css">

<script>
const search = instantsearch({
  appId: 'Q1JJ6NM1UC',
  indexName: 'tigerthinks',
  apiKey: '5a99d8bb6e9d1a03163a5eb0d67496e0',
  searchFunction: function(helper) {
    const searchHits = document.getElementById("search-hits")
    const content = document.getElementById("content")

    if (helper.state.query === '') {
      searchHits.style.display = "none";
      content.style.display = "block";
      return;
    }

    helper.search();
    searchHits.style.display = "block";
    content.style.display = "none";
  },
  searchParameters: {
    hitsPerPage: 10
  }
});

const hitTemplate = function(hit) {
  let date = '';
  if (hit.date) {
    date = moment.unix(hit.date).format('MMM D, YYYY');
  }

  let url = `${hit.url}#${hit.anchor}`;

  const title = hit._highlightResult.title.value;

  let breadcrumbs = '';
  if (hit._highlightResult.headings) {
    breadcrumbs = hit._highlightResult.headings.map(match => {
      return `<span class="post-breadcrumb">${match.value}</span>`
    }).join(' > ')
  }

  const content = hit._highlightResult.html.value;

  return `
    <div class="post-item">
      <span class="post-meta small">${date}</span>
      <a class="post-link" href="${url}"><h2 class="search-hit-title post-title">${title}</h2></a>

      
      <a href="${url}" class="post-breadcrumbs"><h5>${breadcrumbs}</h5></a>
      

      <div class="post-snippet">${content}</div>
    </div>`;
}

// Adding searchbar and results widgets
search.addWidget(
  instantsearch.widgets.searchBox({
    container: '#search-searchbar',
    placeholder: '',
    poweredBy: true,
    autofocus: false,
    cssClasses: {
      root: 'tigerthinks-searchbar'
    }
  })
);


search.addWidget(
  instantsearch.widgets.hits({
    container: '#search-hits',
    templates: {
      item: hitTemplate
    }
  })
);

// Starting the search
search.start();
</script>

<style>
.ais-search-box {
  max-width: 100%;
}

.post-item {
  padding-top: 20px;
  padding-bottom: 20px;
  border-bottom: thin solid #f3f3f3;
}

.post-link .ais-Highlight {
  color: #0076df;
  font-style: normal;
}

.post-breadcrumbs {
  font-style: normal;
  display: block;
  padding-bottom: 10px;
  background-image: none !important;
  color: #333 !important;
}

.post-breadcrumb {
  font-style: normal;
  font-size: 18px;
  color: #333;
}

.post-breadcrumb .ais-Highlight {
  font-weight: bold;
  font-style: normal;
  color: #0076df;
}

.post-snippet .ais-Highlight {
  color: #0076df;
  font-style: normal;
  font-weight: bold;
}

.post-snippet img {
  display: none;
}
</style>

</body>
</html>
